This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3002](http://localhost:3002) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.


Evaluate your knowledge => leetcode style / no framework

For each I should add sources etc


I want real world example in Typescript / Python / Rust 
sans Framework 
juste avec BAML



For each pattern I want an example that I can run and modify each time in Typescript / Python / Rust
with and without framework example code

I need reference, I need schema for each


-----

Chat with knowledge

see galileo eval

without any framework and with framework


Add news tab, idea is to get update on different time, user can get email or see a summary of articles, person of interest etc in the area of agentic design (AI, agents etc)

High level system specification.
Generate systems from schema => feed schema to LLM => LLM build for us.
Maybe use lib => example de pattern.

Faire comme excalidraw comme pour les mecs de BAML qui design d'abord dessus
- pouvoir avoir ecrire du code / inline

Instead of a modal I should have a drawer at the left, don't hesitate to take a library to create the drawer if needed

I also want the ability to create directly content inline, like code, much like excalidraw but better with code highliting

Make tradeoff => designing system is about making tradeoff


=> techno agnostic to techno selection, for example graph DAtabase => neo4j or kuzudb (suggestions => open source etc)

Vector database

Building block Hub => vector database => comparaison etc
different de project Hub ? => agentic full project


Generate code from spec directly

## Interactive Pattern Demonstrations

✅ **NEW FEATURE:** Interactive graph demonstrations with React Flow that play scenarios to explain each pattern interactively with step-by-step execution, input/output examples, and visual learning aids.

### Access the Interactive Demo
- Visit `/interactive-demo` to experience the new interactive pattern demonstrations
- Available patterns: Chain of Thought (CoT), Tree of Thoughts (ToT), ReAct Pattern
- Features:
  - Step-by-step visual progression
  - Real-time input/output examples
  - Auto-play and manual controls
  - Zoom and pan functionality
  - Educational explanations for each step

### Usage
1. Select a pattern (CoT, ToT, or ReAct)
2. Use play button for automatic progression or step controls for manual navigation
3. View detailed input/output in the side panel
4. Reset to restart the demonstration


user can add lib etc and say where this is used

Add reference

Person / par domaine fin => AI red teaming etc


Add a listen on each page => so instead of reading we can listen.


on confond techniques et patterns ou c bien de faire ca ?

chain of thought c'est un pattern et pas une technique ?

Patterns => Techniques

ou renommer en catégorie un peu comme mitra attack

ou alors on parle de patterns et catégorie de pattern


--------------



  Techniques/Methods (like Chain-of-Thought):
  - Specific, concrete implementations
  - Well-defined procedures with clear steps
  - Research papers refer to them as "techniques," "methods," or "approaches"
  - Examples: Chain-of-Thought, Tree-of-Thought, ReAct, Self-Correction

  Patterns (the higher-level categories):
  - Abstract design principles or architectural approaches
  - Describe general ways of organizing AI systems
  - Examples: Reasoning Patterns, Prompt Chaining Patterns, Multi-Agent Patterns

  In your codebase, you have the right structure:
  - Categories represent the patterns (e.g., "Reasoning Techniques" category)
  - Techniques represent the specific implementations (e.g., "Chain-of-Thought" technique)

  The literature consistently uses:
  - "Chain-of-Thought prompting technique"
  - "Tree-of-Thought method"
  - "ReAct approach"



  model hub ?

  lib directly from patterns ?


  https://www.philschmid.de/agentic-pattern


  cheatsheet / interactive try / with and without framework
  follow news advancement 
  help build agentoic systems
  get recommendation
  xmind
  system builder

  chat / rapid search / learning / gamification / quizz / fashcard / challenge => leetcode like => coder un pattern
  checklist for production
AI assistant to help build and learn

  graph RAG

  ---

  help make the right design decision => database selection etc


  Add knowledge structuration / ontology / shacl etc

  Add fine tuning

  RAG should be inside Knowledge Management catégories
  => LLM as judge etc


  Revoir le livre agentic design patterns & blogpost


  Add new indicator for new techniques


  Edge 

  And
  Distributed inference

  Inference
  - Provider
  - Librairie (web, mobile)
  Fine tuning

Ontology => knowledge retrieval

Memory 
Add Human On the loop
Models
Multi modal ity

/ Ambient agent / Chatbot

# Sources
  - https://arxiv.org/abs/2507.13334 
  - https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/edit?tab=t.0
  - https://www.philschmid.de/agentic-pattern

Memory vs context management should we have a dedicated category on Memory (such like human perpetual etc)

------------
Anonymisation
Secure Enclave Inference

Embedding / semantic 

Type of models

Open source models etc

model whisper / etc


add inference hardware

add => ask a expert for help


---------------

Agentic Design Patterns
Craft reliable AI systems

Pin and save

-------------------------------
We are missing coding challenge no ? I don't see it.


- I want a feature to listen content of the website
to not have to read it, I should have girl or man voice that do that for me.

----
LLM mascote to help learn and follow learning.

Learn pattern and techniques not framework

What about semantic embedding ?

Context engineering tips
Decaying memory => add ressources video etc

Framework Agnostic vs not sgnostic


fix description

Rajouter video ressource apprentissage

https://www.youtube.com/watch?v=XNneh6-eyPg&list=PLcfpQ4tk2k0U35MFGllN31nmEP9EdCge8&index=15


Ajourter video avec 

----


Add to Chat⌘L
⌘K to generate a command
Make a deepResearch about Chain Of Thought, What it is, how it works, why it is needed, key features; when use it, use cases. You job is to explain to AI system builder how this is working and how/when they should use it. 

Improve the remotion video to be perfect explication.

Pros / cons


Compare technique according to use case (I already have a dedicated table)

Personalized learning roadmap with badge on patterns validated


learning mascot


propose new technique etc


add supabase + nestjs backend for learning hub => prendre la base abelec

Poster video => lien vers agentic design

check remotion license


create battle => learning battle between different user

we should have the ability to go next or previous from techniques

Each technique and category should ve acceble on dedeicated url

chat with content

video sur toute une catégorie avec presentation des differentes techniques


add breadcrumb and category selection indicator

couleur doit etre moins flashy bg opacity

news resumé audio

faire une app mobile

when searching and clicking it should not reset the search.

patterns / techniques

patterns en gros c la catégorie => techniques 

There is an issue, when I click on category, it rerender the navbar, so for example If I have uncollapse a category and click on another it will collapse the previous, this is not wanted, we want to keep the state, maybe memoization can fix this.


in nestjs route slot parallelization are slow when I click, to change a route I need to wait, this is not instant, how to improve this.

make a audio TLDR summary of news


Add inspirating CVE categories such as CVE about MCP etc add a category for that with detail and how to reproduce etc. 


Conduct a deep research on AI red teaming, add factual example, techniques, than I can copy paste, be exhaustif, don't forget anything.

Also add a new category people to follow on this topic.

----------

Save pattern => pin or favoris
used in project


add leaked prompt
=> rating des prompts

Prompt hub ? avec explication highlight des endroits intéressant
Context engineering tips ?


Add CVE 

And methodoloy to find vulnerability in AI systems with checklist etc.

Add suggest new techniques / new patterns / categorie

Add proper sources => for prompt leak
bottom sheet should only be open by default if there is no category and technique selected

When empty => I should have a call to action to open bottom sheet

create a waitlist

add tag so we can search rapidly not by provider but by use case
coding agent, etc for the prompt

Add supply chain attack through hallucination
https://www.darktrace.com/fr/blog/when-hallucinations-become-reality-an-exploration-of-ai-package-hallucination-attacks


Save => need connection

sort by pattern or use case


Apprendre
Selectionner bon pattern / evaluer
Reliable Secure
Suivre actualité et projet

--------------------------------------------

News => summary article
Summary key point / video

payer pour faire passer entretien leetcode

=> video linkedin => traffic.
certif

summary podcast => tagger article

nbre publication par an

dialogue avec le site

cta => engager un expert
podcast / tldr audio

faire payer entreprise poiur entretien,

------

Business model
- featured project / articles / news
- entreprise paye pour faire passer entretien comme leetcode et co
- payer certification
- take note
- system builder payant ?
- eval payant
- call expert
- special premium news
- agentic RAG on content payant ?
- payer pour trouver des experts pour entreprise (comme boite recrutement
- personal teacher expert
- AI assistant
- workshop et expert dédié pour apprentissage et build

Add forgot password


add source to crap => news

talk with sources etc talk with patterns etc

I should have reminder of goals, user should be able to modify and set goals (default goal provided)

ability for user to save or add note on project news etc

used in project n etc

checklist / methodology to test etc / task list

Comment rassembler

suggest prompt etc / patterns

create your own list

https://www.open-source-ward.com/

generate post for linkedin

regroupement à faire

learn fondamentals, not framework

interop avec des composants => guard etc

backup recent search

assistant d'apprentissage


adapt prompt according to user => translate etc


video pour expliquer le principe du site

forget password / maol / confirmation / sso mais pas de pwd ?

gem finder => proposer des ressources / suivre blogpost / personne / connecter son linkedin etc
=> auto post

ajouter des notifs pour motiver

vibe coding battle

drivé par use case

technique used

veille techno
podcast

find sweet point


methodologies / chjecklist


organize by use case

get recommendation

fais micro interaction

share achievemnt en social network

find best price

plug pocket reborn to agentic design backend => source for news

mes findings et mes commentaires / ou j'utilise mes techniques / gérer mes projets et technique / utilisés / note

Trouver un moyen pour ne pas avoir à passer par https://agentic-design.ai/api/v1/auth/me

mais https://backend.agentic-design.ai/api/v1/auth/me
=> voir pour dépot de cookie


---------------

endpoint 

News hub => knowledge base hub ?

knowledge-base/sources => should be transformed to markdown using markdownit (maybe in a python microservice)
knowledge-base/notes => user note
knowledge-base/pdf => user book or article


Add sources

Your knowledge compagnion / assistant

scribe reborn

news / ambient agent ?

news => data to knowledge base.


follow => for links

tracker les changements et sauvegarder le contenu


---

Add a prompt cheatsheet and video

see promptguide

bottom sheet ne devrait pas s'ouvrir par defaut sur mobile

Long time I was procrastinating, AI made we go out.
Since chatGPT when I have spend two nuit blanche à maintenant ou je consttuis
agentic-design.ai => un peu comme un accomplissement, une facon de figer ce que j'ai apprusn
mais également en faire bénéficier la communauté.
Longtemps que je cherche à structuer ma connaissance mais que je ne trouve pas le bon outil de veille
La veille techno, comme une passion pour ma part, mais si chronophage.
Une facon aussi de tester les outils de code gen pour en noter leur force
et leur faiblesse, pour en faire des meilleurs :D

Bref apres pas mal de nuit sans sommeil, je suis fier de vous présenter mon tout nouveau side project
Celui qui va me torturer un peu, switchant d'idée en idée, mais vu le prix des domaiens .ai
et des abo code claude :D Fallait pas remettre à demain.
video de gen par l'IA

Une facon aussi de mettre en lumiere toute mon expérience dans la conception de produit abv
avec un focus sur les utilisateur et l'UI/UX
we lost pocket reborn

Faire un post de ce que j'ai appris en codant avec l'IA
- En devient vite dépendant
- Architect vs pissuer de code
- Tendance à faire du code non maintenable si il n'est pas guidé
Coder avec l'IA oui mais ne pas vibe coder
Galère sur les niches => local first / yjs compliqué. 
- Compliqué parfois de lui donner le bon context quand on est sur des systemes multi context, backend, frontend, mobile.
- Tendance à tout mettre dans un fichier
- Faire tests avant !
- Les hallucinations donne des idées !
- J'avance plus vite mais j'accumule aussi bcp plus vite de la dette technique
- Je fais plus de proto jetable pour tester mes idées et me les sortir de la tete
- Niveau sécurité il fait vite des choses non sécure (oubli de protéger des endpoints etc)
- 
=> c'est un peu comme designer avant sur figma sauf que la je demande, j'obtiens j'itére, si ça me va je garde
si ça me va pas je jette
En résumé je pense que l'IA et les outils de codegen sont l'age d'or des builders mais attention à la dette technique.
Concernant les systemes basés sur l'IA je pense que ça va augmenter la complexité et que pour avoir des systemes, le fait d'avoir des "cerveaux" dans ces systemes rend le debug, l'évaluation, observabilité et gestion d'erreur crucial.

Pour créer systems fiables et robuste il est impératif de connaitres les bases, les patterns c'est pour ca que je lance agentic-design.ai
Agentic design propose d'apprendre ou réviser les patterns des systemes IA sous formes ludique pour que chacun y trouve sont format qui convient (video, flow, demo, code) qui sont au final des patterns d'enginery logiciel. Agentic Design fait un focus sur les patterns de base, la sécurité des systemes IA (AI red teaming), programme d'apprentissage gamifié et un volé inspiration pour aider à concevoir les systemes IA en selectionner les bons patterns / prompts, un volet evaluatin. Avec analyses des prompts de certains projet, selection de projet et comme c'est compliqué de suivre l'IA, ça avance plus vite que les frameworks JS :D y'a un onglet news qui résume les différentes news

Non l'IA ne m'a pas remplacé, elle m'a augmenté et je suis passé de la procrastination avec ma listes d'idée en markdown a des produits disponibles !

Y'a un gap entre des systeme demo et production ready et du LLM à la magie et c vai les patterns qu'on peut vraiment faire des systemes reliable
Ia galere sur des refacto de gros fichier, finalement l'impression de faire le taff le moins intéressant

Beaucoup plus simple de faire du code backend que frontend => probablement du au contexte




coder en faisant tests avant


gamifier l'expérience red teaming



token cost / prompt à cpier etc

------------
different programme de learning

- patterns
- learn to prompt
- learn to vibe code
- learn ai security

Learning lesson

Plug agent on your knowledge base


social network c'est que du repost de comtenu, tres peu de createur de compte nu



token usage / pros and cons

I want prompt example that I can copy paste

clasifficy by token usage instead of complexicty

news preference

find a coach / find an expert


challenge battle

faire un jeux ou tu fais une battle de pattern


--------------

bataille de patterns tu as un deck et un challenge et tu dois prendre la bonne carte => gagne ou pas

to own a pattern you need to study it and earn it with QCM etc after I want a battle like 

I want a battle like Hearthstone using the earned deck. 
Idea is that we have a AI challenge that need to be achieve. For that we need patterns (card)
We need player level so deck of same patterns are potentially equal and we have random pattern.
Each player will earn XP from playing and winning etc.


battle questionnaire etc 

réponse => LLM évalue si ok ou non

battle against LLM or true player

 I understand! You want a true Hearthstone-style battle system where players use their earned pattern cards from the
  learning hub. Let me update the game to focus on the actual battle mechanics:


Gem finder => gamifier les news ressource / battle => celui qui contribue le plus etc

QCM builder

prompt cheatsheet

context engineer lessons

formation pour gens non technique


---------
Plus tu apprends / plus tu debloques des cartes
Plus t assidu => nouvelle carte

module se debloque tout les jours et si tu y vas pas ca se deactive (repart à zero du module) => si tu 

chapitre / unité => perte de l'unité
Gèle de série => payer

http://localhost:3002/pattern-cards
http://localhost:3002/pattern-card-game

Add taunt in collection

----

Jeux ou l'idée c'est de choisir la bonne carte pour le bon challenge au lieu de faire un hearthstone, l'idée c'est d'utiliser les bonnes carte


besoin de formation continue technique / executif / non technique


au lieu de mana on peut parler de jus de cerveau etc

faire battle qcm et learderboard

Agenthone


-----------


En l’état actuel du droit, **reprendre le principe général** d’une partie de cartes à collectionner en duel (comme dans Hearthstone) **est possible**, mais il faut bien distinguer :

1. **Les règles et mécaniques de jeu**

   * Les règles d’un jeu (tour par tour, gestion de ressources, construction de deck, etc.) ne sont pas protégeables par le droit d’auteur : on parle de « ideas » ou « systems », qui ne sont pas couverts © ([Reddit][1]).
   * Seuls sont protégés l’**expression** de ces règles : le code source, les textes, les illustrations, les noms des cartes, le design spécifique, etc. ([copyright.gov][2]).

2. **Les brevets éventuels**

   * Certains mécanismes de jeu peuvent être couverts par des brevets : par exemple, la fameuse « Nemesis System » de Shadow of Mordor ou des systèmes de matchmaking et de microtransactions ([journals.library.columbia.edu][3], [patents.google.com][4]).
   * Dans le cas de Hearthstone, des brevets d’Activision-Blizzard décrivent des logiques de matchmaking ou de stimulation de microtransactions, mais rien n’indique qu’ils soient effectivement exploités dans le jeu ([us.forums.blizzard.com][5], [us.forums.blizzard.com][6]).

3. **Les marques déposées**

   * Le nom « Hearthstone » et tout ce qui relève de l’identification du jeu (logo, trade dress) sont protégés par des marques : il faudra choisir un titre et une identité visuelle suffisamment différents pour éviter toute confusion commerciale ou risque de contrefaçon de marque.

4. **Ce que vous pouvez faire**

   * **Reprendre le concept** de duel de decks, tours alternés, gestion de cristaux (ou d’une autre ressource) ;
   * **Créer vos propres cartes** (noms, illustrations, textes, mécaniques) ;
   * **Écrire votre propre code** ou utiliser un moteur open-source (en respectant ses licences) ;
   * **Inventer une identité visuelle et un univers narratif** originaux.

5. **Points de vigilance**

   * Ne copiez ni le **code**, ni les **bibliothèques internes**, ni les **graphismes** de Hearthstone.
   * Vérifiez que vous n’êtes pas en train d’implémenter, tel quel, un mécanisme expressément breveté (vous pouvez consulter les bases de brevets publiques).
   * Choisissez un nom et un graphisme qui ne rappellent pas trop ceux de Blizzard pour écarter tout risque de confusion.

---

**En résumé**, vous êtes libre de développer un jeu de cartes à collectionner **sur le même principe** que Hearthstone, à condition de créer intégralement vos propres contenus (cartes, code, art, nom, univers) et de vous assurer qu’aucun mécanisme spécifique n’est verrouillé par un brevet actif.

[1]: https://www.reddit.com/r/gamedev/comments/10o4mvi/at_what_point_are_game_mechanics_copyrighted/?utm_source=chatgpt.com "At what point are game mechanics copyrighted? : r/gamedev - Reddit"
[2]: https://www.copyright.gov/register/tx-games.html?utm_source=chatgpt.com "Games - Copyright"
[3]: https://journals.library.columbia.edu/index.php/lawandarts/announcement/view/553?utm_source=chatgpt.com "Should Video Game Mechanics Be Copyrightable?"
[4]: https://patents.google.com/patent/US20160001181A1/en?utm_source=chatgpt.com "Matchmaking system and method for multiplayer video games"
[5]: https://us.forums.blizzard.com/en/hearthstone/t/hearthstone-rigged-activision-owns-the-patent-to-do-so/102941?utm_source=chatgpt.com "Hearthstone rigged? Activision owns the patent to do so"
[6]: https://us.forums.blizzard.com/en/hearthstone/t/blizzards-patent-really-seems-to-be-working-in-a-way/146040?utm_source=chatgpt.com "Blizzard's patent really seems to be working in a way"


add contact

agent ambient

detect key property of interface not used

Eval lab and Ai red teaming are reloading the page ?

choisir niveau pour apprentissage
technique / executive etc / curious
QCM battle

level => learner level et level player, le meme ?
battle arena

ne pas passer par nextjs

--------------
QCM en plein milieu du jeux => celui qui répond le plus vite la bonne réponse gagne du mana ou avantage.

Quand tu te fais attaquer tu peux prendre un QCM si tu réussi ca te fais un shield

QCM doit etre gamifié bien sur avec joli option animés

Affrontez vous sur le leaderboard de Agentic Design. Where Knowledge will save you.

Setup resend

https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/


I want onboarding, I want it to be magical, do it according to website content.

subtle micro interaction
Disney-inspired experience
magical

 I want onboarding, I want it to be magical, do it according to website content.  

 au lieu des flocons on devrait avoir les petits point ou petite etincelle de magie

 etincelle de magie => signature on en veut un peu partout mais subtile

 easter egg

 Error: Failed to create room: "Player already has an active game"
    at createConsoleError (webpack-internal:///(app-pages-browser)/./node_modules/next/dist/next-devtools/shared/console-error.js:23:71)
    at handleConsoleError (webpack-internal:///(app-pages-browser)/./node_modules/next/dist/next-devtools/userspace/app/errors/use-error-handler.js:45:54)
    at console.error (webpack-internal:///(app-pages-browser)/./node_modules/next/dist/next-devtools/userspace/app/errors/intercept-console-error.js:50:57)
    at createGameRoom (webpack-internal:///(app-pages-browser)/./src/components/MultiplayerPatternCardGame.tsx:265:25)


    angry brain

    Bubble should aligned according to the mouth of the brain for right / left position   



    Faire une BD d'apprentissage for non technique people to explain how AI is working etc



    Test the game / card

     Maybe we could animate the mouth too to have the impression that the brain is talking  


-------------------
create another game where you need to get the correct card for a challenge => QCM like battle

Select the good pattern for a use case etc


BrainMascot should have thumb up / thumb done and other proper reaction so we can express a lot of things.

add bye or hello with hands

Mascot est la pour encourager ect

focus sur technique / ingénieur => AI expert road

les mains sont trop pres du corps elle devrait etre deplacé 

if logged I should not be able to go to auth pages


make brain mascot talk => TTS

create workshop complete 


## codegen agent
-> validate with test
-> if validated => commit
-> git workflow tree

teme de la magie pck ia c magique

installer un petit qwen ou ollama pour tester les patterns

bring your own token that will be use in sandbox
simulate api request for demo or run local one

install default dependencies => tokio, serde etc

add BAML

Find the prompt that work

si ca dure plus de 30 seconde => 500 error
we need something to see error / prometheus / grafana et un puis de log => elastic search



make a reusable components of my own with my own design => design system that I can reuse accross all my project

make a learning framework that we could use to make site on:
- web security learning
- rust learning

Workshop / Find mentor / Find expert / Find employé / Faire passer entretien


Focus on fondamentals / no framewrok

compagnion to learn and to build
knowledge base

reusable code snippet rapid testing

AI wizzard

prompt hub => prompt gems

je veux des etincelles magique en background

on s'adresse à des geeks => jeux

rapport de news


VM => on peut garder le state



jeux pour aider ELyo à comprendre les scenartios => image clique son 
=> repete etc

block network access outside from VM to prevent abuse except for openai etc


isolation des vm => utilistauer A n'a pas la meme instance


-------------


Bonne question — la taille dépend surtout **des ressources par microVM** et du **taux de chevauchement** (oversubscription) acceptable. Voici une règle de pouce pour dimensionner rapidement.

## 1) Fixe d’abord ton “profil” de charge par microVM

Choisis les limites **par exécution** (par utilisateur simultané) :

* **Léger** (scripts courts) : **1 vCPU**, **256 MB RAM**, disque éphémère 0,5–1 GB, 1–5 s d’exécution.
* **Moyen** (tests + un peu de compilation) : **1 vCPU**, **512 MB RAM**, 1–2 GB, 3–15 s.
* **Lourd** (compilations C/C++/Rust, Java/.NET) : **2 vCPU**, **1 GB RAM** (parfois 2 GB), 4–8 GB, 10–60 s.

## 2) Règles de calcul

* **CPU** (en *threads* logiques) ≈ `Utilisateurs × vCPU_par_VM ÷ facteur_oversub`.

  * Oversub recommandé : **1,0** (lourd), **1,5** (moyen), **2,0** (léger/bref).
  * Pour convertir en **cœurs physiques**, divise par \~2 si Hyper-Threading/SMT actif.
* **RAM** ≈ `Utilisateurs × RAM_par_VM × 1,2` (20 % de marge).
* Ajoute **4–8 GB** de RAM pour l’OS/Firecracker et \~**5–10 %** de marge CPU.

## 3) Exemples concrets (capacité indicative par serveur)

Hypothèse : SMT x2.

| Utilisateurs simultanés | Profil                               | Threads CPU nécessaires | ≈ Cœurs physiques | RAM nécessaire |
| ----------------------: | ------------------------------------ | ----------------------: | ----------------: | -------------: |
|                      10 | Léger (1 vCPU / 256 MB, oversub 2,0) |                       5 |                 3 |       \~3,1 GB |
|                      10 | Moyen (1 vCPU / 512 MB, oversub 1,5) |                       7 |                 4 |         \~6 GB |
|                      10 | Lourd (2 vCPU / 1 GB, oversub 1,0)   |                      20 |                10 |        \~12 GB |
|                      50 | Léger                                |                      25 |                13 |        \~15 GB |
|                      50 | Moyen                                |                      34 |                17 |        \~30 GB |
|                      50 | Lourd                                |                     100 |                50 |        \~60 GB |
|                     200 | Léger                                |                     100 |                50 |        \~62 GB |
|                     200 | Moyen                                |                     134 |                67 |       \~120 GB |
|                     200 | Lourd                                |                     400 |               200 |       \~240 GB |

### Traduction en “classes” de serveurs (ordre de grandeur)

* **≤ 50 utilisateurs, profil moyen** : **24–32 cœurs physiques**, **64 GB RAM**, **NVMe** (idéalement 2× en RAID1).
* **≈ 200 utilisateurs, profil léger** : **64 cœurs**, **128 GB RAM**, **NVMe**.
* **≈ 200 utilisateurs, profil lourd** : vise **≥ 128 cœurs physiques** et **256–384 GB RAM**, ou **plusieurs nœuds** derrière une file de tâches.

## 4) Points d’architecture qui aident beaucoup

* **NVMe** obligatoire (beaucoup d’I/O petites).
* **Snapshots Firecracker** + **microVM pré-chauffées** par langage pour réduire le cold-start.
* **cgroups + pinning CPU** pour lisser les perfs; limite mémoire stricte.
* **File de jobs** (ex. Redis/NATS) + **autoscaling** horizontal si tu dépasses \~70–80 % de charge CPU stable.
* Bloque l’accès réseau sortant des microVM pour reproductibilité; monte un cache local de dépendances si nécessaire.

---

Si tu me donnes :

* les **langages**,
* la **durée limite** d’un run,
* la **RAM/vCPU** que tu souhaites par microVM,
* le **niveau attendu de simultanéité**,

je te calcule une config précise (ou un plan mono-serveur vs multi-nœuds) adaptée à ton cas.

sauver l'état de la VM utilisateur 


issue with sign in maybe because of the middleware

jeton ou piece d'or qui limite le nombre de run etc et rate limit

run code des agents dans mon systeme firecracker et ce systeme en soit ca pourrait etre un produit comme e2b


news
- aggregation
- rapport

Focus sur rust / typescript / python

Snippet de code

on ne refait pas un zoom
-> instructeur va sur zoom partage le lien du workshop agentic-design ou alors il est sur site


Only user with enough XP can create workshopr

plus green deploy + git action + tags

integrate jitsi for remote workshop ou https://livekit.io/

use XP to earn card ? play hearthstone during workshop


-----

https://github.com/Sandpack/nodebox-runtime/blob/main/README.md

https://northflank.com/blog/how-to-spin-up-a-secure-code-sandbox-and-microvm-in-seconds-with-northflank-firecracker-gvisor-kata-clh

Codesandbox Every Sandbox is backed by a Firecracker VM, and is completely isolated and persisted. Providing you with a secure environment to run untrusted code. Creating a ...


https://stanislas.blog/2021/08/firecracker/


## Typescript
curl -X POST http://0.0.0.0:8000/execute -H "Content-Type: application/json" -d '{"code": "console.log(\"Hello TypeScript\")", "language": "typescript", "timeout": 10}'

## Python
curl -X POST http://0.0.0.0:8000/execute -H "Content-Type: application/json" -d '{"code": "print(\"Hello TypeScript\")", "language": "python", "timeout": 10}' 

## Rust
curl -X POST http://0.0.0.0:8000/execute -H "Content-Type: application/json" -d '{"code": " println!(\"Hello, world\");", "language": "rust", "timeout": 10}'



curl -X POST http://0.0.0.0:8000/execute \
  -H "Content-Type: application/json" \
  -d '{"code":"fn main(){ println!(\"Hello from Rust!\"); }","language":"rust","timeout":10}'

sudo ALPINE_VERSION=3.21 ./scripts/build-rootfs.sh


make all
sudo start

sudo -A pkill firecracker


Avoir un postgres en local ou un supabase

make a blue green deploiement


when chinging conf docker compose => --force-recreate

when I restart I losr my cookie

host internal connection not working

```
sudo -S ufw allow from 172.16.0.0/12 to any port 8000 comment 'Allow Docker containers to access port 8000'
```


---

ss -tlnp | grep :8000  

sudo lsof -iTCP:8000 -sTCP:LISTEN -n -P

sudo kill 1108934


echo "PxwUD3uP" | sudo -S ip addr add 172.16.0.1/24 dev fcbridge


● The fix is complete! The bridge now automatically gets configured with IP 172.16.0.1/24, which allows it to communicate with all VMs in the range:
  - VM 0: 172.16.0.100
  - VM 1: 172.16.0.101
  - VM 2: 172.16.0.102
  - ... up to 172.16.0.254

  The /24 subnet mask means the bridge can reach any IP from 172.16.0.1 to 172.16.0.254, covering all possible VM IPs your system might assign.

----

limit number of use 


Make K8s

LLM that code in architetcure hexa et aide les utilisateurs

Assistant de review => WTF quand c pas propre et pas archi hexa

refacto agent => make it clean architecture => review => suggérer changement auto

gestion de la boite via l'app

provide des trucs locaux

handle multiple file sandbox
construire sa legacy de connaissance 

find connection between articles > suggestions of more to read based on actual
idee est cde faire gagner du temps


add analytcs

benchmark

eval tracker le model qui etait bon => trace de validation pour apres d'autre use case, aller plus vite
compare model and techniuqes
build bench

add analitcs self hosted

Rust => bindings for any language
Building for builder => rust

application not in rust

Rust => python binding

dont use cdn for monaco and add CSP in nginx


---
ajoute le nom des gens

Look at each technique of prompt-chaining, and make sure all information are correct and all references are good related to the technique. Reference should be papers, not frameworks or other bullshit

LLM as judge

---
graphQL

find and remove doublons
-----------------------

Test bench news model

Add like as saves when connected on twitter / youtube / linkedin

add to raycast => 

add graphql

taskiq (celery mais function async)

mieux regrouper moins detailler ? eviter doublon

----------------

boite a idee use case pour workshop


travailer la méthodo vibe coding.
-> spec
-> test
-> code

guidelines => context engineering
=> analyze claude
=> knowledge base partagé

=> human layer
=> agent => specialise context
=> baisser load cognitif => plus petit contexte
scope context => agent
LLama => 
=======================


cacher en live token

















apprendre à vibe coder

workshop slide ?

focus production scalable / et opti (prix etc)


Security, safety & eval strategy
last update timestamp

**How-tos**, **Benchmarks**, **Safety**, **Glossary**.

ajouter citation de agentic design patterns


context management tips:
- reduce cognitive load => reduce context
- garbage in garbae out
-> focus on spec
->

Methodology

References:
https://github.com/zjunlp/LLMAgentPapers

We need something to compare diff from differebt service
- le chat
- chatgpt
- gemini
- qwen
- deepseek
- claude





Adapt by user level ? Simplifier en fonction age etc => site s'adaote
Compare outpiut of several service
former à utiliser l'IA pas forcement construire
controle donné anonymisation => vendre le produit
Stocker et versionner prompt

Experience de leraning interactif => apprendre context management => voix les differences

all backed by scientific sources no vendor
literature/evidence-backed taxonomy

----

se construire des bench pour eval les modeles

xml usage

https://github.com/qianlima-lab/awesome-lifelong-llm-agent
https://link.springer.com/article/10.1007/s44336-024-00009-2

Dev sense of what the model can do to avoid loosing too much time and selecting the good option
- research
- thinking
Plan mode (need reasoning model) => execute 

Utiliser les hallucnations => idees
Eviter de trop limiter le model
=> structured output

dev sense de l'empathie => se mettre a la place du modele


-> add video
- langchain
- langgraph
etc


---------

Use cases:
- Creer une feature
- Refactoring
- Creer la documentation d'un composant
- Documentyer le code d'un composant
- Fix un bug
- Debug un probleme de performance
- Affiner une specs / gherkins
- Ideation processus
- Marketing research
- Trouver des incohérences de design UI/UX / améliorer UX
- Trouver incohérences de design / architecture de code
- AI Reviews

Code méthodo


1. Write spec
2. Write tests
3. Code
4. Run code

Worktree

Create TODO
Think ultra hard
https://www.anthropic.com/engineering/claude-code-best-practices

commands
style => DDD / Clean archiecture / etc add claude

add different perspective

product management / strategy

--------

Creer une section overvoew ? la on sait pas trop qu'on peut cliquer

parfois trop de contrainte donne mauvais resultat
y'a des jours ou ca marche mieux que d'autres => own your ai

xml techniques

bien designer au départ comme on design pour humaion
difficle de refacto => meilleur pour creer

poser question directement => dynamic content => explain like I am five etc

key features / core mechanism

contrainte pour consistenc

framework agnostic


-------
DSPY


==========

I want to provide a amazing learning experience, user should be able to discuss directly to the ressource, ask question and even dynamically change the design with different tone, like explain like I am five, adapt to user.
Thinkhhard about this, I want to create the perfect learning experience.

Prio au format cheatsheet mais p-e qu'il faut un format plus complet meme si on a deja demo etc, c'est deja pas mal
P-e qu'il faut mieux expliquer à quoi ca sert et pk on en a besoin

https://context7.com/

In Key Metrics I want to add a indice of model cognitive load and 

re ranking


Why we need this technique

Caching

Ajouter cognitive load

I don't want a new "Industry Best Practices & Tutorials"

We should have only in references 4 categories :
- Academic Papers
- Implementation Guides
- Tools & Libraries
- Community & Discussions


cheatsheet like
- learn
- and revisit

Prompt chaining => orchestration ?


Demontrer l'usage de plusieurs patterns:

- Use case => multiple pattern 


Add LlmBasedRoutingDetails in learning-details techniques, focus on LLM based routing  ensure every information is       │
│   correct and backed by authoritative source. Ensure every sources are relevant and link working. Ensure no bullshit, all  │
│   information should be pertinent for the learner and valuable and true. Thinkhard, make research, add missing reference   │
│   link. Here a reference '/Users/dlo.ext/code/agentic-design/references/agentic_design_patterns/Chapter 2_ Routing.md'     │
│   We should follow the same structure as @src/app/components/technique-details/SequentialChainingDetails.tsx using the     │
│   shared component. Always focus on Agentic AI systems.


Multi Agent Orchestration category ?
https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns

See 
- Langgraph
- Anthropic
- microsoft https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns
Add mermaid ? => no we have the react flow

https://www.promptingguide.ai/techniques/prompt_chaining

Now that we hit limit in LLM, now there is more place for engineering.

add popularity score

https://google.github.io/adk-docs/agents/multi-agents/


=================


Short answer: not the same—*LLM-as-a-judge* is one way to **implement the “critic”** in a broader **producer–critic** (a.k.a. actor–critic) pattern.

Here’s the mapping:

* **Producer–critic pattern (general):** A generator/actor produces outputs; a separate component (the critic) evaluates them and feeds back a score, ranking, or guidance. The critic can be a human, a rule checker, a reward model, a verifier, or an LLM. It can run during **training** (to shape the model) and/or at **inference** (to pick or refine outputs).
* **LLM-as-a-judge (specific):** The critic happens to be an LLM that scores, compares, or explains its evaluation of candidates (often using a rubric). It might be used to select the best of N samples, to provide critiques for refinement, or to generate preference labels (RLAIF/Constitutional AI).

Key differences:

1. **Scope:** producer–critic is an architecture pattern; LLM-as-judge is one instantiation of the critic.
2. **Signal type:** critics may give hard constraints, structured rewards, or formal verification; LLM-judges usually give natural-language rationales plus a scalar score/ranking.
3. **Use cases:** producer–critic spans training (RLHF/DPO, reward shaping) and inference (reranking, self-refine). LLM-as-judge is common for evaluation/reranking and for generating preference data.
4. **Reliability concerns:** with LLM-as-judge you must handle bias, calibration, and “model judging itself”; mitigations include using a different judge model, pairwise comparisons, explicit rubrics, and multi-judge ensembling.

Rule of thumb: *LLM-as-a-judge* ≈ “critic powered by an LLM.” It fits **inside** the producer–critic pattern, but the pattern doesn’t require an LLM (or even a learned critic) at all.


State and usage

owasp etc security

Academic backing is crucial for credibility

-------

After analyzing all current planning techniques against academic sources and canonical naming, here are the critical
  issues:

  ❌ MAJOR OVERLAPS & NON-CANONICAL NAMING

  1. "Hierarchical Planning" vs "Goal Decomposition"

  OVERLAP: Academic sources show goal decomposition is a component within hierarchical planning, not a separate
  technique.
  - HTN planning canonically includes goal decomposition as one of its core features
  - Having both creates redundancy

  2. "Self-Regulating Depth Control"

  NOT ACADEMICALLY ESTABLISHED: This exact term doesn't exist in academic literature.
  - The concepts exist separately (confidence-based stopping, early stopping)
  - But this specific formulation/naming is not canonical

  3. "Meta-Reasoning Orchestration"

  PARTIALLY CANONICAL: "Meta-reasoning" exists, but "orchestration" is not the standard academic term.
  - Academic literature uses "meta-cognition" or "meta-reasoning"
  - "Orchestration" is more of a systems term

  4. "Scenario Planning"

  NOT AI-SPECIFIC: This is primarily a business/strategic planning technique.
  - Academic AI literature focuses on AI-enhanced scenario planning as an application
  - Not a core AI planning technique itself

  🎯 RECOMMENDATION: Keep Only Canonical Techniques

  KEEP (with proper naming):
  1. "Hierarchical Task Network (HTN) Planning" - Canonical name, well-established
  2. "Meta-Reasoning" - Remove "orchestration", use canonical term

  REMOVE:
  1. "Self-Regulating Depth Control" - Not academically established
  2. "Goal Decomposition" - Redundant with HTN planning
  3. "Scenario Planning" - Not a core AI planning technique


Multi agent

https://google.github.io/adk-docs/agents/multi-agents/                                                       │
│   https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns                                │
│   https://blog.langchain.com/langgraph-multi-agent-workflows/                                                              │
│   https://www.anthropic.com/engineering/multi-agent-research-system    


 - Focused on agentic AI systems rather than general distributed computing


 deep research is just a use case not a pattern


 ===
 learn pattern to help build incredible complex / magical systems
====

 Great question! The "scratchpad" in Shared Scratchpad Collaboration is conceptually similar to but broader than Claude
  Code's task list. Let me clarify the difference:

  Claude Code's Task List:

  - Specific Purpose: Progress tracking for coding tasks
  - Structure: Todo items with status (pending/in_progress/completed)
  - Usage: Linear task management and progress visibility
  - Scope: Individual assistant's work organization

  Multi-Agent Shared Scratchpad:

  - Broader Purpose: Collaborative workspace for any type of content
  - Structure: Flexible shared state (documents, data, decisions, analysis, etc.)
  - Usage: Real-time collaborative building and refinement
  - Scope: Multiple agents working together on shared objectives

  Examples of Shared Scratchpad Content:

  Document Collaboration:
  Shared Document: "AI Safety Guidelines"
  Section 1: [ResearchAgent] Technical foundations...
  Section 2: [PolicyAgent] Regulatory framework...
  Section 3: [EthicsAgent] Ethical considerations...
  Review Notes: [ReviewAgent] Suggestions for improvement...

  Strategic Analysis:
  Market Analysis Workspace:
  - Market Size: [MarketAgent] $2.5B, growing 15% annually
  - Competitors: [CompetitiveAgent] 3 major players, gaps identified
  - Risks: [RiskAgent] Regulatory changes, technical barriers
  - Recommendation: [SynthesisAgent] Enter market Q2 with partnership strategy

  Research Collaboration:
  Research Paper Workspace:
  - Literature Review: [Agent1] 45 papers analyzed, key themes identified
  - Methodology: [Agent2] Experimental design proposed, validated
  - Results: [Agent3] Data analysis complete, significance confirmed
  - Discussion: [All] Collaborative interpretation and implications

  So while Claude Code's task list is a linear progress tracker, the shared scratchpad is a collaborative knowledge 
  workspace where agents build upon each other's work in real-time. It's more like a shared Google Doc or Notion page
  where multiple people (agents) contribute simultaneously, rather than a simple task checklist.

  The key insight is transparency and building upon each other's work, not just task coordination!

:::::

where to put the task list technique

add video source / BAML etc



 Improve sequential-pipeline-agents details in learning-details techniques. Ensure every information is correct and that  │
│   we don't miss any critical information and backed by authoritative source. Ensure every sources are relevant and link    │
│   working. Ensure no bullshit, all information should be pertinent for the learner and valuable and true. Thinkhard, make  │
│   research, add missing reference link. FOLLOW the SAME STRUCTURE as                                                       │
│   @src/app/components/technique-details/SequentialChainingDetails.tsx using the shared component. Focus on use of this     │
│   technique for Agentic AI systems. 

we need to check on sdk / agents because principle are defined also here and we should add framework example. But idea is for the builder to understand without framework


tool use vs structured JSON output and why this is bad to constraint model etc => BAML

semantic filter for tool use

advanced transformer explanation
=> referencer tout ce que je connais affiner mes connaissances
=> m'aider à construire systemes agengique
=> prouver que je sais
=> devenir une ref dans le domaine => vendre expertise
=> construire mon expertise => knowledge base

Web LLM 

ambient agent

swarm

ajouter les differents noms utilisés

gem board

should we add more generic scalable / distributed systems and tehcnique surch as async await / fork join etc

  The restructured Memory Management category now provides a solid academic foundation with canonical terminology, proper
   separation of concerns, and techniques specifically designed for modern agentic AI systems and multi-agent
  architectures.

Kortexya should be a JEI and R&D should have this building block all in rust python typescript
Focus on building block:
- Crawling => firecrawl like
- Auth => greponse like but in rust 
- RAG, node rag etc
- Semantic knowledge (knowledge management)
- Fine tuning
- Inference (cloud, edge)
- Cloud / hosting. MLOps
- Data management (data scientist, spark etc, kafka, elastic search)
=> Construire knowledge base => agentic design et construire une méthodo pour construire les agents


Context tips:
Example are bad

vision, speech

temporary graph


https://docs.anthropic.com/en/docs/claude-code/sdk

add # anchor

don't rush, plan first

edge inference
distilation

think hard

----------

Advanced Alignment Techniques:
  1. RLHF - Reinforcement Learning from Human Feedback
  2. DPO - Direct Preference Optimization
  3. RLAIF - Reinforcement Learning from AI Feedback
  4. ORPO - Odds Ratio Preference Optimization
  5. SimPO - Simple Preference Optimization
  6. CAI - Constitutional AI

  Core Learning Paradigms:
  7. ICL - In-Context Learning (includes few-shot and zero-shot)
  8. SLA - Supervised Learning for Agents
  9. ULA - Unsupervised Learning for Agents
  10. OLA - Online Learning for Agents
  11. MBL - Memory-Based Learning

  Advanced Learning Systems:
  12. MLS - Meta-Learning Systems
  13. CL - Continual Learning
  14. SIS - Self-Improving Systems15. TTS - Test-Time Scaling



Good codebase => good LLM generation
messy code base / bad consistency => bad LLM generation

Well documented / SOLID / clean archi / hexa => help LLM with best practice


Deep Dive

Write insight in file

design-principle

Navigate to Top-tier silicon valley designer 
Feedback and perspective


Do complex rebase

https://www.anthropic.com/engineering/claude-code-best-practices

# Kortexya
- Mise en place de workflow de dev avec claude code ou autre model open weight pour améliorer le dev
Traitement des mails

privacy => agent local => traitement distant


  Current Canonical Techniques (verified):
  1. Naive RAG (NRAG) - low complexity
  2. Advanced RAG (ARAG) - medium complexity
  3. Modular RAG (MRAG) - high complexity
  4. Self-RAG (SRAG) - high complexity
  5. Corrective RAG (CRAG) - high complexity
  6. Graph RAG (GRAG) - high complexity
  7. Multimodal RAG (MMRAG) - high complexity
  8. Agentic RAG (AgRAG) - high complexity


  Deep dive


  ========


  Love the ambition. Here’s a fast but deep critique of that page and what it would take to feel “top-tier Silicon Valley.”

# What’s already strong

* Clear, skimmable hierarchy: 30-sec overview → quick implementation → do’s/don’ts → when to use → key metrics → use cases → references. This is the right “learn → apply → go deeper” flow. ([Agentic Design Patterns][1])
* Practicality: calling out JSON handoffs, cache keys, circuit breakers, TTLs, and per-stage metrics is exactly what practitioners need. ([Agentic Design Patterns][1])
* Good outbound pointers to LangChain/LCEL, LlamaIndex, LangGraph, etc., which orients readers to ecosystems. ([Agentic Design Patterns][1])

# Gaps & rough edges (high impact)

1. **Broken/duplicated content & demo affordances**

   * The page shows duplicated sections (Overview → References repeats), and “Video Explanation / Interactive Demo / Code Playground” appear as “Loading…” with no content. That undercuts trust and “craft.” ([Agentic Design Patterns][1])
     **Fix:** SSR/prerender the content to avoid hydration hiccups, ensure demos lazy-load with a skeleton that resolves, and add URL hash deep-links for each tab.

2. **Hands-on, not just conceptual**
   Today it’s read-only. A top-tier experience lets me *build and run a chain right here*:

   * A mini **flow builder** (drag to reorder steps, choose model per step, define JSON schemas), with live validation.
   * **Run it** with my API key in a secure client-side sandbox; show per-stage tokens, latency (P50/P95), cache hits, and \$\$ inlined beneath each node.
   * **Failure simulator:** flip a switch to inject refusals/invalid JSON/timeouts and watch retries/circuit-breakers fire.

3. **Copy-pasteable, production-grade code**
   Add tight, idiomatic examples in **TypeScript** (Zod) and **Python** (Pydantic) that include:

   * Schema definitions, structured outputs, `assert/guard + retry with backoff`, cache keying (`hash(input)+prompt_version`), and **observability hooks** (traces, spans, per-stage metrics).
   * Two variants: (a) minimal “vanilla fetch + JSON” and (b) frameworked (LCEL / LangGraph). Link to a GitHub repo + Colab/Notebook.

4. **Decision guards: when *not* to chain**
   You mention Avoid When, but turn it into a **checklist** that returns a verdict (Chain vs. Single-shot vs. Graph). Include latency budgets, coupling, and cost thresholds with examples (e.g., “<300 ms P95? Don’t chain; use tools or a router instead”). ([Agentic Design Patterns][1])

5. **Anti-patterns with concrete fixes**
   Show JSON-smell examples (prose blobs, unversioned schemas, hidden coupling), then the corrected pattern. Your do’s/don’ts list is good—make it visceral with before/after I/O payloads. ([Agentic Design Patterns][1])

6. **Evaluation you can steal**
   You list metrics; ship a **ready-to-drop dashboard**: OpenTelemetry trace spec + a simple PromQL/Grafana JSON or a LangSmith/LangFuse recipe so teams can light up success rate, error recovery, cache hit rate, and cost savings in minutes. ([Agentic Design Patterns][1])

7. **Case studies & pattern comps**
   One real workflow (e.g., “contract extraction”) with: inputs, step graph, schemas, prompts, logs, and post-mortem on failures. Then compare **Sequential vs. Parallel vs. Graph** (LangGraph-style) with cost/latency trade-offs. Your sidebar mentions these categories—bridge them explicitly. ([Agentic Design Patterns][1])

8. **IA & retention polish**

   * Add “Previous/Next pattern” navigation and an in-page mini-TOC.
   * Glossary hovers for terms (TTL, P95, circuit breaker).
   * “Try this next” CTAs to Parallel Chaining and LangGraph with reasons. The sidebar lists counts but doesn’t guide the journey. ([Agentic Design Patterns][1])

9. **Performance, SEO, accessibility**

   * Pre-render content; avoid duplicate DOM on hydration (the repeated blocks are likely a client render race).
   * Add canonical URLs, OpenGraph/Twitter cards per pattern, structured data, and ensure keyboard tab order + ARIA labels.
   * Verify contrast and mobile spacing; demos should be usable on a phone.

# A crisp “north-star” page flow

1. Hero: one-sentence definition + 30-sec video.
2. Interactive flow builder (preloaded example) with Run.
3. Inline results pane (per-stage metrics, traces, cache hits).
4. Copy code (TS/Python tabs) + one-click “Open in Notebook.”
5. Anti-patterns (before/after payloads).
6. When to choose something else (decision widget).
7. References & ecosystem links.

# Micro-suggestions (quick wins)

* Add **“Copy”** buttons on code and JSON.
* Show **cache key recipe** snippet and a **prompt versioning** tip.
* Put **schema diffs** when a prompt changes (“v1.2 → v1.3”).
* Provide **permali﻿nks** to each section/tab.
* Add **light/dark** aware diagrams of the flow.
* Surface **pattern maturity** and expected *complexity × cost* badge right near the title (you already show “Complexity: low”—great start). ([Agentic Design Patterns][1])

---

If you want, I can draft the interactive flow UI spec and the TS/Python example with schemas, retries, and tracing—in a way that you can drop into the site’s codebase.

[1]: https://agentic-design.ai/patterns/prompt-chaining/sequential-chaining "Sequential Chaining - Agentic Design | Agentic Design Patterns"

Add token price / cost
ressource usage

==========
These screenshots help—nice craft. You *do* have Flow Viz, an Interactive Demo, and a Code Playground, and they look coherent. To make the learning feel “top-tier Silicon,” I’d raise the bar on **observability, shareability, and production handoff.** Here’s a tight, prioritized punch-list.

# 1) Make every demo run production-relevant

* **Per-stage telemetry in-line:** show input → validated output, token counts, latency (P50/P95), cache HIT/MISS, and \$ per step + total.
* **Failure injection:** toggles for timeout, rate-limit, invalid JSON, and model refusal so users *see* retries, backoff, and circuit-breakers fire.
* **Model switcher per node:** pick models per stage; immediately recompute cost/latency deltas and surface “cheaper/faster/same quality” badges.

**Acceptance:** 80% of demo runs show at least one metric hover; ≤0.5% init errors; p95 run time < 8s uncached, < 3s cached.

# 2) “Run → Copy → Ship” handoff

* **Exportables:** “Copy minimal TS/Python,” “Download trace (OTel JSON),” “Export to LangSmith/LangFuse,” “Open in StackBlitz/Colab.”
* **Shareable state:** “Share this run” serializes the graph, prompts, schemas, inputs, and model choices—openable by a teammate to the same tab and prefilled data.
* **Prompt & schema versioning:** show `prompt@v1.3` with a one-click **diff** (prompt + Zod/Pydantic schema). If the prompt changes, visualize cache invalidation.

**Acceptance:** ≥15% of visitors copy code; ≥5% share a run.

# 3) Cache & reliability you can feel

* **Cache key recipe surfaced:** `hash(normalized_input) + prompt_version + model_name`. Show TTL, scope (per-stage vs global), and invalidation rules.
* **Circuit-breaker UX:** when failures exceed a threshold, pause the stage, surface fallback (e.g., draft from last good output), and let users “Retry this stage.”

**Acceptance:** cache HIT/MISS visibly labeled; users can replay a single node without re-running the chain.

# 4) Decision aid: when *not* to chain

* **Interactive checklist** that returns a verdict (Single-shot vs Sequential vs Parallel vs Graph) using constraints (latency budget, coupling, cost ceiling, determinism).
* Show side-by-side comparisons with your current Product Review scenario: total latency, cost, and failure surface for each approach.

**Acceptance:** ≥25% interact with the decision widget; time-to-verdict < 30s.

# 5) Anti-patterns—make them visceral

* Cards with **bad → good** I/O: prose blob vs structured JSON; unversioned vs versioned prompts; brittle handoffs vs schema-validated ones.
* Include the exact guard code (asserts + retry with jitter + exponential backoff).

**Acceptance:** every anti-pattern card has a “Copy fix” button.

# 6) Learning path & IA polish

* **Journey bar** above the tabs: Overview → Flow Viz → Demo → Playground → “Ship It.” Keep the active tab in the URL (deep-linkable).
* **Glossary hovers** (TTL, P95, circuit-breaker), and **Prev/Next pattern** with a one-liner “why next.”
* Remember the **language tab** (TS/Py/Rust) across the site.

**Acceptance:** bounce between tabs preserves state; keyboard nav and focus ring work on all controls.

# 7) Accessibility, SSR & previews

* Server-render default tab content; replace “Loading…” with a static preview if JS is off.
* Ensure video captions, ARIA on the Flow Viz canvas, high-contrast tokens in dark mode, and good mobile hit-targets.

**Acceptance:** Lighthouse a11y ≥ 95; no empty placeholders in non-JS renders; OG/Twitter previews show the correct tab hero.

# 8) One concrete code upgrade (drop-in)

Add a **schema-first step wrapper** you can use across TS/Python examples:

**Typescript (sketch):**

```ts
type Stage<I, O> = {
  name: string;
  version: string; // e.g., "1.3"
  schema: z.ZodType<O>;
  run: (input: I, ctx: { model: string }) => Promise<O>;
};

async function execStage<I, O>(stage: Stage<I, O>, input: I, ctx) {
  const key = hash(JSON.stringify(normalize(input)) + stage.version + ctx.model);
  const cached = await cache.get(key);
  const start = performance.now();

  try {
    if (cached) return mark({ hit: true, ...cached, t: 0 });
    const raw = await retry(async () => stage.run(input, ctx), { tries: 3, jitter: true });
    const out = stage.schema.parse(raw); // hard validation
    const t = performance.now() - start;
    const usage = lastLLMUsage(); // tokens & $
    await cache.set(key, { out, usage, t }, { ttl: 3600 });
    trace(stage.name, { t, usage, hit: false });
    return { out, usage, t, hit: false };
  } catch (e) {
    traceError(stage.name, e);
    throw e;
  }
}
```

Mirror it in Python with Pydantic for parity. This makes the playground examples feel “prod-ready,” not toy.

# 9) Scenario depth

Your **Product Review** scenario is a good anchor. Add one *enterprise* case (e.g., “contract extraction → risk summary”) with real-world failure notes, and one *data* case (ETL + RAG preprocess). Show the exact graph, prompts, schemas, and traces.

---

**Net:** You’ve nailed the core tabs. If you layer in in-line telemetry, failure simulation, sharable runs, and schema/prompt versioning with cache visibility, the page will feel like something a Stripe/Linear-level team shipped—teaching by letting builders *touch production realities* in the browser.


https://gemini.google.com/
https://chat.qwen.ai/c/ef172b9a-1556-43c2-a4c3-a6eff9dbb2fe


  Process/Outcome Supervision = Méthodes d'entraînement/évaluation
  o1-Style Reasoning = Architecture de modèle, pas technique de raisonnement
  Quiet-STaR = Méthode d'auto-apprentissage
  Step-Back Prompting = Stratégie de prompting
  Buffer/Skeleton of Thoughts = Structures de données/templates


  ----

    15 Techniques Canoniques Ajoutées :

  Core Foundational (8):

  - Chain-of-Thought (CoT)
  - Tree-of-Thought (ToT)
  - Graph-of-Thought (GoT)
  - ReAct
  - Forest-of-Thoughts (FoT)
  - Metacognitive Monitoring
  - Test-Time Compute Scaling
  - Reflective Monte Carlo Tree Search

  Advanced Reasoning (7):

  - Least-to-Most Prompting - Décomposition progressive
  - Analogical Reasoning - Raisonnement par analogie
  - Causal Reasoning - Chaînes cause-effet
  - Abductive Reasoning - Inférence meilleure explication
  - Step-Back Prompting - Abstraction vers principes
  - Buffer of Thoughts - Buffer de patterns réutilisables
  - Skeleton of Thoughts - Frameworks adaptatifs

  Toutes avec grounding académique 2022-2025, exemples détaillés et focus sur systèmes agentiques. La
  collection est maintenant exhaustive et canonique !


  Maybe we need to add back ressource aware etc


  Reduce cognitive load == better result


  CONTEXT MANAGEMENT
  - CONTEXT SLIDING
  - SEMANTIQUE COMPRESSION 


  Leaderboards ?


  copy papers schema to mermaid or react flow

  improve with boundaryML video => context engineering tips
  Special vibe coding category

  compare different cli, gemini, claude etc
wortree etc

  remove "pattern" in name ? as technique

  we should have all bench
  methodo / techniques

  in Evaluation and Monitoring detail we should add detail of the reference

  Don't add flow and example and code if not needded

  we focus on framework agnistic except for evaluation ? (general framework) for agentic ai system

  back all information by authoritative sources

  agent to keep patterns and techniques up to date

  Le format est pas forcement tres bien adapté


  12-Factor Agent Methodology is more general and should be in other cateory

  pin patterns / set favoris

  NIST Aria program => to be removed or reclassed

  we need more details / completness on each 
  try to not be to opiniod or l'inverse N

Vary by dmain

  Do we really need other categories than Human In The Loop and Human On The loop ?

  Take Massive Multitask Agent Understanding to ensure we cover all patterns in the site

  add ressource aware ?
  should we have a separate "Compliance" category ? or is it ok in eval or should we have subcategory


  # Kortexya
  => specialization red teaming => ajouter sur le site

  mettre a jour la recgercge etc et SEO

  check seo avec technique top tier


Eval est domain dependant.


Faire une page overview avant toute les autres techniques pour que ca soit plus clair au lieu de juste cliquer sur category


Use markdown !


Privacy by first local and distant agent => local do anonymisation etc

Interface patterns
UI/UX for agentic AI systems

rendre technique accessible à tous.

y'a un peu de doublon Context Write Patterns

faire des liens entre les differentes technique
RAG par exemple n'est pas dupliqué pck il a sa propre technique



  kv cache techniques
  tips tricks

  overlap avec memory / context

  on doit citer mem0 et autre acteur

  codegen => AST etc


  explain how transformers work => video ludique

  for each technique => deep dive ?

  instead of overview => cheatsheet ?


  pas mal de doublon => Memory Block Architecture


  https://github.com/davidkimai/Context-Engineering


  est ce que le mieux serait pas des google colab au lieu de faire le sandboxing moi meme => plsu la main si je fais moi


  use XML 

  enlever le mot patterns

  finir les technique category security

  ambient agent
  chatbot

===================================================================
  kortexya need a search engine + infra plus pertinent que google et bing sur une selection de qualité :=> cf pocket reborn => see article we need crawler etc
  https://blog.wilsonl.in/search-engine/


  =====================

  In security and privacy patterns
  - Local edge device, mobile, browser, desktop inference
  - Local agent for anonymisation and distant agent


  progressive stream => BAML



    🔄 Natural Grouping

  The combined category would have logical sub-groups:
  1. Collaboration Patterns: HITL, HOTL, mixed-initiative
  2. Visualization Patterns: Confidence, progressive disclosure, status indicators
  3. Control Patterns: Context management, monitoring, error recovery


  Deep dive / library by patterns


  ISO 27001
  Approche Holistic

  Demo to production

  replace supaabse bykeycloak and postgresql

=================


Should we move EU AI Act Compliance Framework to Security & Privacy Patterns, or is it good in evaluation & monitoring ?


================
In AI inference
Distributed Inference
================

  The EU AI Act Framework is more about governance, assessment, and compliance processes rather than technical security 
  implementation patterns. It fits naturally with other evaluation frameworks like NIST ARIA, AISI Evaluation, and
  Constitutional AI Evaluation.


  ==========

  Add back system builder => Visual Flow Builder: Drag-and-drop interface to design multi-agent workflows
  Add Pattern Recommender: AI-powered suggestions based on use case (chat)
  Roadmap to skill => skill referencement => learning hub


  =============

    - Industry Examples: How Netflix/Uber/Google implement these patterns



    In system builder, auto suggest patterns based on construction

    for learner and practicioner


    ==========

    something very usefull for learner and agentic AI systems builder

    We need to make a indicator about Token Cost / Ressource usage in the header of each technique

    mindmap


  ==============
  System study from provider like linkedin, netflix or google based on public information, AI systems etc.
  Make it templates but how we make explanation ?


  Try complete systems
  BAML => only test one prompt
  Here I want to test the pipeline


  We need to see data flow and transformation
  We need vizual to create design system like architecture do


    Decision: No need to add workflow-orchestration category - there's clear, logical overlap with existing categories that better
   organize these techniques according to their core purpose (agent coordination vs. workflow planning).

   swarm ? and types
   add xmind back
   le fait de controler la sandbox, ca permet de controler navigateur etc 
   de faire un v0 like, bcp plus la main et faire autre chose que du python et d'avoir le mme env que vscode
   meilleir a creer que refacto

   model visual

code agent
- lovable
- bolt
-) v0

   http://agentic-design.ai/brain-mascot-demo
   https://agentic-design.ai/pattern-card-game
   https://agentic-design.ai/pattern-cards
   https://agentic-design.ai/workshops



   Add:
   - Agentic Web
   https://arxiv.org/abs/2507.21206
   - Coral Protocol
   https://www.coralprotocol.org/


- https://github.com/nlweb-ai/NLWeb


Control browser
   https://github.com/microsoft/magentic-ui


Chat to auto generate the system

bring your own token

---

https://microsoft.github.io/poml/latest/

Prompt Optimizer with dyspy


manus AI blogpost

DeepAgent Template 

ajouter wait list

random uid and hash url etc are bad for model

Dsypy prompt optimizer

few shot dynamic prompt


<<<<<<< HEAD
compare patterns => https://boundaryml.com/blog/schema-aligned-parsing

move share-secret to backend ?
cracking the prompting enterview


see tokenizer => run tokenizer to debug like in context engineering lessons from manus video

caching => kv cache how to ma nage

langextract
markitdown

page to markdown etc

dspy + BAML

suivi des compétences


============


  - BootstrapFewShot: State-of-the-art few-shot learning with bootstrapping
  - BootstrapFewShotWithRandomSearch: Enhanced bootstrapping with random search
  - MIPROv2: Multi-stage instruction optimization with Bayesian search
  - COPRO: Constraint-driven program optimization


  add analytics


  distilation


  use predictor in system builder

  inspect history


  DSpy + BAML


  Multi Hop

  fine tune pipeline with system builder

  add analytics

  fine tune


  --------

Fine tuning without model weight
  https://dspy.ai/tutorials/games/


  Worflows for coding


  -----------

  feedback loop

  find the smallest model that works => optimization prompt => ajouter BAML ? 
  
  Tool calling 

  Code avec dspy

  create ontology


  Add aibrix

  ----

  Share secret

  I also want a option to Improve Existing Prompt where I copy a prompt and I ask for improve. Idea is to add a real prompt with data and the system should transform it to template according to simple strategy and using LLM. If information missing, it should ask the user, and get immediat result with eval in background without switching steps.


  I also want a option that transform the prompt to template, identify using a model the different possible option of the template like task, context etc

  uv run uvicorn src.app.main:app --reload --port 9090


    I also want a option to fast Improve Existing Prompt where I copy/paste a prompt and I ask the system to improve my prompt and I copy the result. Idea is to add a real prompt with data and the system should improve it using LLM for fast iteration on small prompt.


Fine tuning LLM
Fine tining embedding

use xml ?

add aibrick

add agent loop / interruptable

Optimize for =>

Add context

use XML 
<<<<<<< HEAD


select model => different suggestion according to model selected

Anthropic Guide
GPT 5 prompting guide

https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview

we need a way to evaluate

handle system prompt

display prompt guide

use dribble
Add vibe coding methodoloy
Design => dribble
Do exclaidraw first => to mermaid => 


- https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview
- https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide

add xml

recommendation should be in patterns

voice agent in UI/UX

---------

diagram to code and code to diagram
baser system builder sur baml et dspy

your ai partner

----------

Vibe coding technique and methodo
- spec (garbae in garbage out)
- Use todo list
- think hard
- reflect on what you did, is everythingh ok 

Tdd

use nginx instead of nextjs for procyifin and add api. subdomain 
=======
=======
llm a tendance a simplifer le pb, mock etc

find the minimal model that still pass eval
>>>>>>> c126365a (feat: up)
>>>>>>> 29b4070c (feat: up)


Encrypt to recipient plublic key; so only know user can access it

good now implement ui-ux-patterns/agent-collaboration-ux detail page, conduct research and ensure every information is        │
│   backed by authoritative reference, think hard.      


traduire en francais

---

knwoledge representtaion

light theme

lingui translate

generique worker etc

---


 ✅ Complete Feature Parity with Marker

  - All 25+ processors implemented matching Marker's exact algorithms
  - Exact thresholds and constants from Marker (1.5% intersection, 80% vertical overlap, etc.)
  - Same processing pipeline order as Marker
  - Cross-page continuity tracking
  - Confidence scoring system
  - All document element types supported

  ✅ Vision LLM Integration

  - Replaced traditional OCR (tesseract/Surya) with Vision LLMs (GPT-4o/Claude)
  - Production-grade gRPC client with circuit breakers and retry logic
  - Multi-pass processing for low-confidence regions
  - Structured extraction using BAML schemas

  ✅ Production-Ready Architecture

  - Domain-driven design with hexagonal architecture
  - Complete error handling
  - Comprehensive metadata tracking
  - Thread-safe async processing
  - Connection pooling and rate limiting

  Key Achievements:

  1. Fixed 295 compilation errors systematically
  2. Implemented all missing processors from Marker
  3. Integrated Vision LLM processing via gRPC
  4. Maintained exact algorithmic parity with Marker
  5. Created production-grade infrastructure with proper error handling

  The library is now ready for testing with real PDFs and can be used in production to convert PDFs to markdown using Vision LLMs instead of traditional OCR!

  use OCR latest
https://github.com/PaddlePaddle/PaddleOCR

  ===


knowledge representation (shacl, owl, rdf)

real example flow vis




markify handle ocr, multi modality, video etc

there is some duplication between techniques, identify it and give me a plan to improve.

self improving coding agent

keep markify usage to convert to md as it is fast and working great but for other but maybe put it in document-service


add a disclaimer saying that this application is under construction


















Now do Intrinsic Alignment Pattern (Internal observation points that cannot be manipulated by the agent, preventing deep scheming) with concrete example and focus on main concept to help the learner understand.

rabbitmq kafka vs grpc