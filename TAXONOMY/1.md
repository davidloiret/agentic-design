I want you to improve my following taxonomy of patterns / techniques about Agentic AI systems.  Make a deep research on latest patterns and techniques on academic and authoritative sources, use canonical names, for each technique you should find a verifiable source from academic paper. Find gaps in patterns / techniques and remove redundant one. Use industry adoption term and academic term. The goal is to create a comprehensible reference for agentic AI systems engineers in order to help learn how to craft reliable, secure and scalable agentic AI systems. 

# Prompt Chaining
- Sequential Chaining
- Parallel Chaining

# Routing
- LLM-based Routing
- Embedding-based Routing
- Rule-based Routing
- Machine Learning Model-Based Routing

# Reflection
- Self-Critique 
- LLM as judge

# Tool use
- Structured Output 
- Function Calling
- Code Execution
- Model Context Protocol

# Planning
- Hierarchical Planning
- Goal Decomposition
- Scenario Planning
- Deep research

# Multi-Agent Collaboration
- A2A Protocol (Agent2Agent)
- Sequential Handoffs
- Parallel Processing
- Debate and Consensus
- Hierarchical Structures
- Expert Teams
- Critic-Reviewer

# Memory Management
- Short-Term Memory (Contextual Memory)
- Long-Term Memory (Persistent Memory) 

# Context Management
- Adaptive Context Depth
- Latent Knowledge Retrieval
- Advanced Context Compression
- Multimodal Context Integration
- Sliding Window
- Hierarchical Memory
- Attention Mechanisms
- Context Compression (semantic, etc)
- Multi-Source Context Fusion

# Learning and Adaptation
- Reinforcement Learning
- Supervised Learning
- Unsupervised Learning
- Few-Shot/Zero-Shot Learning with LLM-Based Agents
- Online Learning
- Memory-Based Learning
- Proximal Policy Optimization (PPO)
- Direct Preference Optimization (DPO)

# Goal Setting and Monitoring
- ...

# Exception Handling and Recovery
- Error Detection & Handling
- Recovery
- Circuit Breaker Pattern
- Intelligent Retry with Backoff
- Graceful Degradation
- Comprehensive Health Monitoring

# Human-AI Collaboration
- Human-in-the-Loop (Feedback Loops)
- Human On the Loop

# Knowledge Retrieval (RAG)
- Classical RAG (Chunking of Documents, Embeddings, Text Similarity / COS distance, Semantic Similarity and Distance, Vector database)
- Graph RAG
- Self-RAG
- Corrective RAG
- Adaptive RAG
- Modular RAG
- Multimodal RAG
- Conversational RAG
- Hierarchical RAG
- Chain-of-Verification RAG
- Agentic RAG Systems

# Resource-Aware Optimization
- Adaptive Compute Scaling
- Cost-Aware Model Selection
- Energy-Efficient Inference
- Memory Optimization
- Latency Optimization

# Reasoning techniques
- Chain-of-Thought (CoT)
- Tree-of-Thought (ToT)
- Self-correction
- Program-Aided Language Models (PALMs)
- Reinforcement Learning with Verifiable Rewards (RLVR)
- ReAct
- Chain of Debates
- Graph of Debates
- MASS (optional advanced topic)

# Guardrails/Safety Patterns
- ...

# Evaluation and Monitoring
- Holistic Capability & Risk Tracking (multi-metric) — HELM as living benchmark (accuracy, robustness, bias, efficiency). https://arxiv.org/abs/2211.09110?utm_source=chatgpt.com
- Agent/Tool-Use Benchmarks — AgentBench, WebArena, OSWorld for interactive, tool-rich settings. https://arxiv.org/abs/2308.03688
- LLM-as-Judge Metrics — MT-Bench / Chatbot Arena for scalable quality judgments. https://arxiv.org/abs/2306.05685
- Latency Monitoring
- Tracking Token Usage for LLM Interactions
- Custom Metric for "Helpfulness" using LLM-as-a-Judge

# Prioritization
- Multi-Criteria Weighted Scoring
- Multi-Criteria Decision Analysis
- Dynamic Priority Queue Systems
- Dynamic Content Ranking

# Exploration and Discovery
- Reinforcement Learning Exploration
- Curiosity-Driven Exploration
- Multi-Armed Bandit Optimization
- Evolutionary Discovery Algorithms

Focus on latest papers from 2022 - aout 2025. Give me the result in markdown in the same form as what I provided to you.

Write result to result.md


role based



Agentic web
stable diffusion


-----------

Offload context => write to file
=> task files

Context enginnering is the process of deciding what inforamtion to feed into the LLM's context window to guide output.
- scratchpad
- memories

RAGAs


prompt injection is an encoding context probleme just like XSS

Special vibe coding:
- git wortree
- subagent etc

claude code

MCP => claude code / playwhrit

specialiser context

plan: opus 4.1
execute: sonnet

claude --continue

shift + tab => plan / auto accept 

point d'exclamation
# ==> add to memory

think step by step

human layer / 12 factors

command

partager claude inter equip
worktrees
--dangerous-skip-permission

/mcp

playwright

- npx @playwright/mcp@latest
- claude mcp add playwright npx '@playwright/mcp@latest'
context7

masterclass claude code 10x dev Melvynx


-------

anchors