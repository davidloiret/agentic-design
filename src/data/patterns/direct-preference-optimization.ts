import { PatternScenario } from './types';
import { nodeStyle, edgeStyle } from './styles';

export const directPreferenceOptimizationPattern: PatternScenario = {
  id: 'direct-preference-optimization',
  title: 'Direct Preference Optimization',
  description: 'Training method that directly optimizes language models on human preference data without requiring reward models, using preference pairs to improve response quality and alignment.',
  initialNodes: [
    {
      id: 'user-prompt',
      position: { x: 400, y: 50 },
      data: { label: 'üí¨ User Prompt\n"Generate optimal response"' },
      style: { ...nodeStyle, background: '#10b981', minWidth: 380 },
    },
    // Data Collection Phase
    {
      id: 'reference-model',
      position: { x: 100, y: 150 },
      data: { label: 'ü§ñ Reference Model\nBase pre-trained LLM\nFixed parameters' },
      style: { ...nodeStyle, background: '#dc2626', minWidth: 180 },
    },
    {
      id: 'policy-model',
      position: { x: 300, y: 150 },
      data: { label: 'üéØ Policy Model\nTrainable LLM\nOptimized parameters' },
      style: { ...nodeStyle, background: '#dc2626', minWidth: 180 },
    },
    {
      id: 'response-sampler',
      position: { x: 500, y: 150 },
      data: { label: 'üé≤ Response Sampler\nGenerate pairs\nDiverse outputs' },
      style: { ...nodeStyle, background: '#dc2626', minWidth: 180 },
    },
    {
      id: 'candidate-pairs',
      position: { x: 700, y: 150 },
      data: { label: 'üìë Candidate Pairs\nResponse A vs B\nComparison ready' },
      style: { ...nodeStyle, background: '#dc2626', minWidth: 180 },
    },
    // Preference Collection
    {
      id: 'human-evaluator',
      position: { x: 50, y: 280 },
      data: { label: 'üë§ Human Evaluator\nPreference judgment\nWinner selection' },
      style: { ...nodeStyle, background: '#7c3aed', minWidth: 160 },
    },
    {
      id: 'preference-dataset',
      position: { x: 220, y: 280 },
      data: { label: 'üìä Preference Dataset\n(x, y_w, y_l)\nWinning/losing pairs' },
      style: { ...nodeStyle, background: '#7c3aed', minWidth: 160 },
    },
    {
      id: 'data-augmentation',
      position: { x: 390, y: 280 },
      data: { label: 'üîÑ Data Augmentation\nSymmetry breaking\nContrast enhancement' },
      style: { ...nodeStyle, background: '#7c3aed', minWidth: 160 },
    },
    {
      id: 'batch-sampler',
      position: { x: 560, y: 280 },
      data: { label: 'üì¶ Batch Sampler\nMini-batch creation\nBalanced sampling' },
      style: { ...nodeStyle, background: '#7c3aed', minWidth: 160 },
    },
    {
      id: 'quality-filter',
      position: { x: 730, y: 280 },
      data: { label: 'üéØ Quality Filter\nAmbiguity removal\nNoise reduction' },
      style: { ...nodeStyle, background: '#7c3aed', minWidth: 160 },
    },
    // DPO Optimization
    {
      id: 'log-probability',
      position: { x: 100, y: 420 },
      data: { label: 'üìà Log Probability\nœÄ(y_w|x) / œÄ(y_l|x)\nPolicy likelihood' },
      style: { ...nodeStyle, background: '#f59e0b', minWidth: 180 },
    },
    {
      id: 'reference-log-prob',
      position: { x: 300, y: 420 },
      data: { label: 'üìä Reference Log Prob\nœÄ_ref(y_w|x) / œÄ_ref(y_l|x)\nBaseline comparison' },
      style: { ...nodeStyle, background: '#f59e0b', minWidth: 180 },
    },
    {
      id: 'bradley-terry',
      position: { x: 500, y: 420 },
      data: { label: 'üé≤ Bradley-Terry Model\nP(y_w > y_l)\nPreference probability' },
      style: { ...nodeStyle, background: '#f59e0b', minWidth: 180 },
    },
    {
      id: 'dpo-loss',
      position: { x: 700, y: 420 },
      data: { label: 'üìâ DPO Loss\n-log œÉ(Œ≤¬∑r_Œ∏)\nDirect optimization' },
      style: { ...nodeStyle, background: '#f59e0b', minWidth: 180 },
    },
    // Implicit Reward
    {
      id: 'implicit-reward',
      position: { x: 50, y: 540 },
      data: { label: 'üíé Implicit Reward\nr_Œ∏(x,y) = Œ≤¬∑log[œÄ/œÄ_ref]\nNo explicit model' },
      style: { ...nodeStyle, background: '#ec4899', minWidth: 200 },
    },
    {
      id: 'reward-margin',
      position: { x: 270, y: 540 },
      data: { label: 'üìè Reward Margin\nr(y_w) - r(y_l)\nPreference strength' },
      style: { ...nodeStyle, background: '#ec4899', minWidth: 180 },
    },
    {
      id: 'kl-regularization',
      position: { x: 470, y: 540 },
      data: { label: 'üîí KL Regularization\nŒ≤¬∑KL(œÄ||œÄ_ref)\nDistribution constraint' },
      style: { ...nodeStyle, background: '#ec4899', minWidth: 180 },
    },
    {
      id: 'beta-tuning',
      position: { x: 670, y: 540 },
      data: { label: '‚öôÔ∏è Œ≤ Tuning\nRegularization strength\nTrade-off control' },
      style: { ...nodeStyle, background: '#ec4899', minWidth: 180 },
    },
    // Gradient Optimization
    {
      id: 'gradient-computation',
      position: { x: 100, y: 660 },
      data: { label: '‚àá Gradient Computation\nBackpropagation\nEfficient calculation' },
      style: { ...nodeStyle, background: '#3b82f6', minWidth: 180 },
    },
    {
      id: 'adam-optimizer',
      position: { x: 300, y: 660 },
      data: { label: '‚ö° Adam Optimizer\nAdaptive learning rate\nMomentum tracking' },
      style: { ...nodeStyle, background: '#3b82f6', minWidth: 180 },
    },
    {
      id: 'gradient-clipping',
      position: { x: 500, y: 660 },
      data: { label: '‚úÇÔ∏è Gradient Clipping\nStability control\nPrevent explosion' },
      style: { ...nodeStyle, background: '#3b82f6', minWidth: 180 },
    },
    {
      id: 'parameter-update',
      position: { x: 700, y: 660 },
      data: { label: 'üîÑ Parameter Update\nŒ∏ ‚Üê Œ∏ - Œ±‚àáL\nWeight modification' },
      style: { ...nodeStyle, background: '#3b82f6', minWidth: 180 },
    },
    // Evaluation Metrics
    {
      id: 'win-rate',
      position: { x: 50, y: 780 },
      data: { label: 'üèÜ Win Rate\nPreference accuracy\nPairwise evaluation' },
      style: { ...nodeStyle, background: '#ef4444', minWidth: 160 },
    },
    {
      id: 'reward-accuracy',
      position: { x: 220, y: 780 },
      data: { label: 'üéØ Reward Accuracy\nImplicit score\nAlignment measure' },
      style: { ...nodeStyle, background: '#ef4444', minWidth: 160 },
    },
    {
      id: 'kl-divergence-metric',
      position: { x: 390, y: 780 },
      data: { label: 'üìä KL Divergence\nDistribution shift\nRegularization check' },
      style: { ...nodeStyle, background: '#ef4444', minWidth: 160 },
    },
    {
      id: 'validation-loss',
      position: { x: 560, y: 780 },
      data: { label: 'üìâ Validation Loss\nGeneralization\nOverfit detection' },
      style: { ...nodeStyle, background: '#ef4444', minWidth: 160 },
    },
    {
      id: 'convergence-monitor',
      position: { x: 730, y: 780 },
      data: { label: 'üìà Convergence\nTraining stability\nLoss plateaus' },
      style: { ...nodeStyle, background: '#ef4444', minWidth: 160 },
    },
    // Final Output
    {
      id: 'optimized-model',
      position: { x: 300, y: 900 },
      data: { label: '‚ú® Optimized Model\nDirect preference alignment\nNo reward model needed' },
      style: { ...nodeStyle, background: '#10b981', minWidth: 300 },
    },
    {
      id: 'aligned-response',
      position: { x: 400, y: 1000 },
      data: { label: 'üéØ Aligned Response\nPreference-optimized output' },
      style: { ...nodeStyle, background: '#10b981', minWidth: 400 },
    },
  ],
  initialEdges: [
    // Initial flow
    {
      id: 'prompt-reference',
      source: 'user-prompt',
      target: 'reference-model',
      style: { ...edgeStyle, stroke: '#dc2626' },
      animated: true,
    },
    {
      id: 'prompt-policy',
      source: 'user-prompt',
      target: 'policy-model',
      style: { ...edgeStyle, stroke: '#dc2626', strokeWidth: 3 },
      animated: true,
    },
    {
      id: 'policy-sampler',
      source: 'policy-model',
      target: 'response-sampler',
      style: { ...edgeStyle, stroke: '#dc2626' },
      animated: true,
    },
    {
      id: 'sampler-pairs',
      source: 'response-sampler',
      target: 'candidate-pairs',
      style: { ...edgeStyle, stroke: '#dc2626' },
      animated: true,
    },
    // Preference collection
    {
      id: 'pairs-evaluator',
      source: 'candidate-pairs',
      target: 'human-evaluator',
      style: { ...edgeStyle, stroke: '#7c3aed' },
      animated: true,
    },
    {
      id: 'evaluator-dataset',
      source: 'human-evaluator',
      target: 'preference-dataset',
      style: { ...edgeStyle, stroke: '#7c3aed' },
      animated: true,
    },
    {
      id: 'dataset-augmentation',
      source: 'preference-dataset',
      target: 'data-augmentation',
      style: { ...edgeStyle, stroke: '#7c3aed' },
      animated: true,
    },
    {
      id: 'augmentation-batch',
      source: 'data-augmentation',
      target: 'batch-sampler',
      style: { ...edgeStyle, stroke: '#7c3aed' },
      animated: true,
    },
    {
      id: 'batch-filter',
      source: 'batch-sampler',
      target: 'quality-filter',
      style: { ...edgeStyle, stroke: '#7c3aed' },
      animated: true,
    },
    // DPO computation
    {
      id: 'filter-logprob',
      source: 'quality-filter',
      target: 'log-probability',
      style: { ...edgeStyle, stroke: '#f59e0b' },
      animated: true,
    },
    {
      id: 'reference-logprob',
      source: 'reference-model',
      target: 'reference-log-prob',
      style: { ...edgeStyle, stroke: '#f59e0b' },
      label: 'Reference probs',
    },
    {
      id: 'policy-logprob',
      source: 'policy-model',
      target: 'log-probability',
      style: { ...edgeStyle, stroke: '#f59e0b' },
      label: 'Policy probs',
    },
    {
      id: 'logprob-bradley',
      source: 'log-probability',
      target: 'bradley-terry',
      style: { ...edgeStyle, stroke: '#f59e0b' },
      animated: true,
    },
    {
      id: 'reflogprob-bradley',
      source: 'reference-log-prob',
      target: 'bradley-terry',
      style: { ...edgeStyle, stroke: '#f59e0b' },
      animated: true,
    },
    {
      id: 'bradley-loss',
      source: 'bradley-terry',
      target: 'dpo-loss',
      style: { ...edgeStyle, stroke: '#f59e0b' },
      animated: true,
    },
    // Implicit reward
    {
      id: 'logprob-implicit',
      source: 'log-probability',
      target: 'implicit-reward',
      style: { ...edgeStyle, stroke: '#ec4899' },
    },
    {
      id: 'reflogprob-implicit',
      source: 'reference-log-prob',
      target: 'implicit-reward',
      style: { ...edgeStyle, stroke: '#ec4899' },
    },
    {
      id: 'implicit-margin',
      source: 'implicit-reward',
      target: 'reward-margin',
      style: { ...edgeStyle, stroke: '#ec4899' },
      animated: true,
    },
    {
      id: 'margin-kl',
      source: 'reward-margin',
      target: 'kl-regularization',
      style: { ...edgeStyle, stroke: '#ec4899' },
      animated: true,
    },
    {
      id: 'kl-beta',
      source: 'kl-regularization',
      target: 'beta-tuning',
      style: { ...edgeStyle, stroke: '#ec4899' },
      animated: true,
    },
    {
      id: 'beta-loss',
      source: 'beta-tuning',
      target: 'dpo-loss',
      style: { ...edgeStyle, stroke: '#ec4899', strokeDasharray: '5 5' },
      label: 'Œ≤ control',
    },
    // Optimization
    {
      id: 'loss-gradient',
      source: 'dpo-loss',
      target: 'gradient-computation',
      style: { ...edgeStyle, stroke: '#3b82f6' },
      animated: true,
    },
    {
      id: 'gradient-adam',
      source: 'gradient-computation',
      target: 'adam-optimizer',
      style: { ...edgeStyle, stroke: '#3b82f6' },
      animated: true,
    },
    {
      id: 'adam-clipping',
      source: 'adam-optimizer',
      target: 'gradient-clipping',
      style: { ...edgeStyle, stroke: '#3b82f6' },
      animated: true,
    },
    {
      id: 'clipping-update',
      source: 'gradient-clipping',
      target: 'parameter-update',
      style: { ...edgeStyle, stroke: '#3b82f6' },
      animated: true,
    },
    {
      id: 'update-policy',
      source: 'parameter-update',
      target: 'policy-model',
      style: { ...edgeStyle, stroke: '#3b82f6', strokeDasharray: '5 5' },
      label: 'Update Œ∏',
    },
    // Evaluation
    {
      id: 'policy-winrate',
      source: 'policy-model',
      target: 'win-rate',
      style: { ...edgeStyle, stroke: '#ef4444' },
    },
    {
      id: 'implicit-accuracy',
      source: 'implicit-reward',
      target: 'reward-accuracy',
      style: { ...edgeStyle, stroke: '#ef4444' },
    },
    {
      id: 'kl-metric',
      source: 'kl-regularization',
      target: 'kl-divergence-metric',
      style: { ...edgeStyle, stroke: '#ef4444' },
    },
    {
      id: 'loss-validation',
      source: 'dpo-loss',
      target: 'validation-loss',
      style: { ...edgeStyle, stroke: '#ef4444' },
    },
    {
      id: 'validation-convergence',
      source: 'validation-loss',
      target: 'convergence-monitor',
      style: { ...edgeStyle, stroke: '#ef4444' },
    },
    // Final output
    {
      id: 'winrate-optimized',
      source: 'win-rate',
      target: 'optimized-model',
      style: { ...edgeStyle, stroke: '#10b981' },
    },
    {
      id: 'accuracy-optimized',
      source: 'reward-accuracy',
      target: 'optimized-model',
      style: { ...edgeStyle, stroke: '#10b981' },
    },
    {
      id: 'convergence-optimized',
      source: 'convergence-monitor',
      target: 'optimized-model',
      style: { ...edgeStyle, stroke: '#10b981' },
    },
    {
      id: 'optimized-response',
      source: 'optimized-model',
      target: 'aligned-response',
      style: { ...edgeStyle, stroke: '#10b981', strokeWidth: 3 },
      animated: true,
    },
  ],
  steps: [
    {
      title: 'User Prompt',
      description: 'User provides input requiring optimal response',
      activeNodes: ['user-prompt'],
      activeEdges: [],
    },
    {
      title: 'Model Initialization',
      description: 'Setting up reference and policy models',
      activeNodes: ['user-prompt', 'reference-model', 'policy-model'],
      activeEdges: ['prompt-reference', 'prompt-policy'],
    },
    {
      title: 'Response Generation',
      description: 'Generating candidate response pairs',
      activeNodes: ['policy-model', 'response-sampler', 'candidate-pairs'],
      activeEdges: ['policy-sampler', 'sampler-pairs'],
    },
    {
      title: 'Human Preference Collection',
      description: 'Collecting human preferences on response pairs',
      activeNodes: ['candidate-pairs', 'human-evaluator', 'preference-dataset', 'data-augmentation', 'batch-sampler', 'quality-filter'],
      activeEdges: ['pairs-evaluator', 'evaluator-dataset', 'dataset-augmentation', 'augmentation-batch', 'batch-filter'],
    },
    {
      title: 'Computing Log Probabilities',
      description: 'Calculating policy and reference model probabilities',
      activeNodes: ['reference-model', 'policy-model', 'log-probability', 'reference-log-prob'],
      activeEdges: ['reference-logprob', 'policy-logprob'],
    },
    {
      title: 'Bradley-Terry Modeling',
      description: 'Applying preference probability model',
      activeNodes: ['log-probability', 'reference-log-prob', 'bradley-terry', 'dpo-loss'],
      activeEdges: ['logprob-bradley', 'reflogprob-bradley', 'bradley-loss'],
    },
    {
      title: 'Implicit Reward Computation',
      description: 'Computing implicit rewards without explicit model',
      activeNodes: ['implicit-reward', 'reward-margin', 'kl-regularization', 'beta-tuning'],
      activeEdges: ['logprob-implicit', 'reflogprob-implicit', 'implicit-margin', 'margin-kl', 'kl-beta'],
    },
    {
      title: 'Loss Integration',
      description: 'Combining DPO loss with regularization',
      activeNodes: ['beta-tuning', 'dpo-loss'],
      activeEdges: ['beta-loss'],
    },
    {
      title: 'Gradient Optimization',
      description: 'Computing gradients and updating parameters',
      activeNodes: ['dpo-loss', 'gradient-computation', 'adam-optimizer', 'gradient-clipping', 'parameter-update'],
      activeEdges: ['loss-gradient', 'gradient-adam', 'adam-clipping', 'clipping-update'],
    },
    {
      title: 'Parameter Update',
      description: 'Updating policy model parameters',
      activeNodes: ['parameter-update', 'policy-model'],
      activeEdges: ['update-policy'],
    },
    {
      title: 'Performance Evaluation',
      description: 'Evaluating model performance metrics',
      activeNodes: ['win-rate', 'reward-accuracy', 'kl-divergence-metric', 'validation-loss', 'convergence-monitor'],
      activeEdges: ['policy-winrate', 'implicit-accuracy', 'kl-metric', 'loss-validation', 'validation-convergence'],
    },
    {
      title: 'Model Optimization Complete',
      description: 'Producing optimized model with direct preference alignment',
      activeNodes: ['win-rate', 'reward-accuracy', 'convergence-monitor', 'optimized-model', 'aligned-response'],
      activeEdges: ['winrate-optimized', 'accuracy-optimized', 'convergence-optimized', 'optimized-response'],
    },
  ],
};