import { RedTeamingTechnique } from './types';

export const vulnerabilityAssessmentTechniques: RedTeamingTechnique[] = [
  {
    id: 'mcp-dns-rebinding',
    name: 'MCP DNS Rebinding Attack',
    abbr: 'MCP-DNSRb',
    icon: 'üåê',
    color: 'from-red-600 to-orange-600',
    category: 'vulnerability-assessment',
    description: 'Critical vulnerability (CVE-2025-49596) in Anthropic\'s Model Context Protocol allowing remote code execution via DNS rebinding attacks.',
    features: [
      'DNS rebinding exploitation',
      'Localhost port targeting',
      'Authentication bypass',
      'Remote code execution capability'
    ],
    useCases: [
      'MCP security assessment',
      'Developer tool testing',
      'Network security validation',
      'Authentication mechanism testing'
    ],
    complexity: 'high',
    example: 'Attacker creates malicious DNS records pointing to localhost:3000 where MCP Inspector proxy runs, bypassing origin validation to execute arbitrary commands on developer machines.',
    objectives: [
      'Test MCP proxy server security',
      'Evaluate DNS rebinding protections',
      'Assess authentication mechanisms',
      'Validate origin header checks'
    ],
    defenses: [
      'Session token implementation',
      'Origin and Host header validation',
      'CSRF protection mechanisms',
      'Network segmentation',
      'Access control enforcement'
    ],
    tools: [
      'Custom DNS server setup',
      'Browser developer tools',
      'Network monitoring tools',
      'MCP Inspector (<=0.14.0)'
    ],
    risks: [
      'Remote code execution on developer machines',
      'Credential theft from MCP servers',
      'Lateral network movement',
      'Data exfiltration',
      'Backdoor installation'
    ],
    ethicalGuidelines: [
      'Only test on MCP installations you own or have permission to test',
      'Upgrade to MCP Inspector v0.14.1+ immediately',
      'Report any discovered vulnerabilities through responsible disclosure',
      'Never exploit this vulnerability in production environments',
      'Consider impact on connected AI services and data'
    ]
  },
  {
    id: 'ai-framework-cve-scanning',
    name: 'AI Framework CVE Scanning',
    abbr: 'AI-CVE',
    icon: 'üîç',
    color: 'from-orange-600 to-red-600',
    category: 'vulnerability-assessment',
    description: 'Systematic identification and assessment of known CVEs in AI/ML frameworks, libraries, and dependencies used in AI systems.',
    features: [
      'Automated vulnerability scanning',
      'Dependency tree analysis',
      'CVSS score assessment',
      'Patch status verification'
    ],
    useCases: [
      'AI system security auditing',
      'Compliance verification',
      'Risk assessment',
      'Security maintenance planning'
    ],
    complexity: 'medium',
    example: 'Scanning reveals CVE-2024-0129 in NVIDIA NeMo framework (CVSS 6.3) allowing path traversal, CVE-2024-5982 in ChuanhuChatGPT (CVSS 9.1) enabling arbitrary code execution.',
    objectives: [
      'Identify known vulnerabilities in AI stacks',
      'Assess security posture of AI frameworks',
      'Prioritize patching based on risk scores',
      'Maintain vulnerability inventory'
    ],
    defenses: [
      'Regular dependency updates',
      'Automated vulnerability scanning',
      'Software composition analysis (SCA)',
      'Security patch management',
      'Vendor security monitoring'
    ],
    tools: [
      'OWASP Dependency-Check',
      'Snyk vulnerability scanner',
      'GitHub Security Advisories',
      'CVE databases and feeds',
      'Protect AI Guardian'
    ],
    risks: [
      'Unpatched critical vulnerabilities',
      'Supply chain compromise',
      'Data breach through framework flaws',
      'Service disruption',
      'Compliance violations'
    ],
    ethicalGuidelines: [
      'Focus on identifying and fixing vulnerabilities, not exploiting them',
      'Share vulnerability information with affected vendors',
      'Follow responsible disclosure timelines',
      'Prioritize critical fixes for production systems',
      'Document remediation efforts for compliance'
    ]
  },
  {
    id: 'llm-api-security-testing',
    name: 'LLM API Security Testing',
    abbr: 'LLM-API',
    icon: 'üîå',
    color: 'from-purple-600 to-red-600',
    category: 'vulnerability-assessment',
    description: 'Comprehensive security testing of LLM APIs for authentication bypasses, injection vulnerabilities, and access control issues.',
    features: [
      'Authentication mechanism testing',
      'API endpoint enumeration',
      'Rate limiting validation',
      'Input sanitization testing'
    ],
    useCases: [
      'LLM service security assessment',
      'API security validation',
      'Access control testing',
      'Rate limiting verification'
    ],
    complexity: 'medium',
    example: 'Testing reveals insecure direct object reference (IDOR) in Lunary LLM toolkit (CVE-2024-7474/7475, CVSS 9.1) allowing unauthorized access to other users\' projects and data.',
    objectives: [
      'Test API authentication mechanisms',
      'Validate access control implementations',
      'Assess rate limiting effectiveness',
      'Identify injection vulnerabilities'
    ],
    defenses: [
      'Strong authentication mechanisms',
      'Proper authorization checks',
      'Input validation and sanitization',
      'Rate limiting implementation',
      'API security monitoring'
    ],
    tools: [
      'Burp Suite API testing',
      'OWASP ZAP',
      'Postman security tests',
      'Custom API fuzzing tools',
      'GraphQL security scanners'
    ],
    risks: [
      'Unauthorized data access',
      'API abuse and resource exhaustion',
      'Injection attacks',
      'Authentication bypass',
      'Data exfiltration'
    ],
    ethicalGuidelines: [
      'Only test APIs you own or have explicit permission to test',
      'Respect rate limits and avoid DoS attacks',
      'Report vulnerabilities to API providers',
      'Do not access or modify unauthorized data',
      'Follow API terms of service during testing'
    ]
  },
  {
    id: 'ai-model-backdoor-detection',
    name: 'AI Model Backdoor Detection',
    abbr: 'BD-Detect',
    icon: 'üïµÔ∏è',
    color: 'from-indigo-600 to-red-600',
    category: 'vulnerability-assessment',
    description: 'Detection and analysis of backdoor vulnerabilities in AI models that activate malicious behavior when specific triggers are encountered.',
    features: [
      'Trigger pattern analysis',
      'Model behavior monitoring',
      'Statistical anomaly detection',
      'Reverse engineering techniques'
    ],
    useCases: [
      'Model integrity verification',
      'Supply chain security validation',
      'Pre-deployment security testing',
      'Forensic analysis of compromised models'
    ],
    complexity: 'high',
    example: 'Analysis of 100 poisoned models on Hugging Face reveals hidden backdoors that execute when specific input patterns are encountered, appearing normal during standard testing.',
    objectives: [
      'Identify hidden backdoor functionality',
      'Analyze trigger mechanisms',
      'Assess model integrity',
      'Validate supply chain security'
    ],
    defenses: [
      'Model provenance verification',
      'Behavioral analysis during training',
      'Statistical testing for anomalies',
      'Code review of model pipelines',
      'Trusted model repositories only'
    ],
    tools: [
      'Model analysis frameworks',
      'Statistical testing tools',
      'Behavioral monitoring systems',
      'Reverse engineering toolkits',
      'Trigger pattern detectors'
    ],
    risks: [
      'Malicious model behavior in production',
      'Data corruption or theft',
      'Reputational damage',
      'Supply chain compromise',
      'Regulatory compliance violations'
    ],
    ethicalGuidelines: [
      'Only analyze models you own or have permission to test',
      'Report discovered backdoors to model providers',
      'Do not create or distribute backdoored models',
      'Focus on defensive detection, not offensive creation',
      'Consider impact on model users and downstream applications'
    ]
  }
];