import { Technique } from './types';

export const interpretabilityTechniques: Technique[] = [
  {
    id: 'latent-space-visualization',
    name: 'Latent Space Visualization',
    abbr: 'LSV',
    icon: 'üî¨',
    color: 'from-violet-500 to-purple-600',
    category: 'interpretability',
    description: 'Visualize and interpret AI reasoning processes in continuous latent spaces',
    features: [
      'High-dimensional space mapping',
      'Reasoning trajectory visualization',
      'Thought evolution tracking',
      'Semantic clustering display',
      'Interactive exploration tools',
      'Multi-modal latent analysis'
    ],
    useCases: ['research-analysis', 'ai-debugging', 'model-understanding', 'reasoning-audit'],
    complexity: 'high',
    example: 'Latent Reasoning Visualization:\n\nProblem: "Plan a sustainable city"\n\nVisualization Output:\n‚Ä¢ 3D latent space map showing reasoning clusters\n‚Ä¢ Energy systems cluster (green)\n‚Ä¢ Transportation cluster (blue)\n‚Ä¢ Housing cluster (orange)\n‚Ä¢ Connection strength indicators\n‚Ä¢ Reasoning path: Energy ‚Üí Transport ‚Üí Integration\n\nInsights Revealed:\n‚Ä¢ Model prioritizes energy before transport\n‚Ä¢ Strong coupling between housing and energy\n‚Ä¢ Weak consideration of economic factors\n‚Ä¢ Bias toward technical over social solutions\n\nUser Benefits:\n‚Ä¢ Understand AI reasoning patterns\n‚Ä¢ Identify potential biases\n‚Ä¢ Debug unexpected outputs\n‚Ä¢ Validate reasoning quality'
  },
  {
    id: 'contrastive-explanations',
    name: 'Contrastive Explanations',
    abbr: 'CE',
    icon: '‚öñÔ∏è',
    color: 'from-blue-500 to-cyan-600',
    category: 'interpretability',
    description: 'Explain AI decisions by showing what would change the outcome',
    features: [
      'Counterfactual analysis',
      'Decision boundary exploration',
      'Alternative pathway generation',
      'Minimal change identification',
      'Feature importance ranking',
      'Causal relationship mapping'
    ],
    useCases: ['decision-support', 'bias-detection', 'model-validation', 'user-trust'],
    complexity: 'medium',
    example: 'Loan Approval Decision:\n\nOriginal Decision: "Loan Denied"\nReason: Credit score too low (580)\n\nContrastive Explanation:\n"Your loan was denied, but it would be approved if:\n‚Ä¢ Credit score increased to 620 (+40 points)\n‚Ä¢ OR annual income increased to $75K (+$15K)\n‚Ä¢ OR down payment increased to 25% (+10%)\n\nSmallest Change Needed:\n‚Ä¢ Increase credit score by 40 points\n‚Ä¢ This would change decision from DENY to APPROVE\n\nOther factors that didn\'t matter:\n‚Ä¢ Employment history (sufficient)\n‚Ä¢ Debt-to-income ratio (acceptable)\n\nActionable Steps:\n1. Pay down credit card balances\n2. Check credit report for errors\n3. Consider authorized user status"'
  },
  {
    id: 'causal-reasoning-transparency',
    name: 'Causal Reasoning Transparency',
    abbr: 'CRT',
    icon: 'üîó',
    color: 'from-emerald-500 to-green-600',
    category: 'interpretability',
    description: 'Make causal reasoning chains explicit and understandable',
    features: [
      'Causal chain visualization',
      'Cause-effect relationship mapping',
      'Confounding factor identification',
      'Intervention effect prediction',
      'Causal strength quantification',
      'Assumption transparency'
    ],
    useCases: ['scientific-reasoning', 'policy-analysis', 'medical-diagnosis', 'business-strategy'],
    complexity: 'high',
    example: 'Medical Diagnosis Reasoning:\n\nSymptom: "Patient has chest pain and shortness of breath"\n\nCausal Reasoning Chain:\n1. Chest Pain + Shortness of Breath\n   ‚Üì [Causal Link: 0.85 strength]\n2. Possible Cardiac Event\n   ‚Üì [Mediating Factors: Age (60), Male, Smoker]\n3. Risk Assessment: HIGH\n   ‚Üì [Intervention Effect: -70% risk if treated within 1 hour]\n4. Recommendation: Immediate Emergency Care\n\nCausal Assumptions Made:\n‚Ä¢ Age increases cardiac risk (literature-based)\n‚Ä¢ Gender affects risk profile (demographic data)\n‚Ä¢ Time-to-treatment affects outcomes (clinical studies)\n\nConfounding Factors Considered:\n‚Ä¢ Exercise-induced symptoms (ruled out: at rest)\n‚Ä¢ Anxiety symptoms (possible, but cardiac priority)\n‚Ä¢ Medication side effects (none reported)\n\nTransparency: All causal links have evidence sources'
  },
  {
    id: 'attention-flow-analysis',
    name: 'Attention Flow Analysis',
    abbr: 'AFA',
    icon: 'üëÅÔ∏è',
    color: 'from-orange-500 to-red-600',
    category: 'interpretability',
    description: 'Track and visualize where AI systems focus their attention during reasoning',
    features: [
      'Attention pattern visualization',
      'Focus intensity mapping',
      'Temporal attention tracking',
      'Multi-head attention analysis',
      'Cross-modal attention flows',
      'Attention anomaly detection'
    ],
    useCases: ['model-debugging', 'bias-detection', 'reasoning-validation', 'performance-optimization'],
    complexity: 'medium',
    example: 'Document Analysis Attention:\n\nTask: "Summarize this research paper"\n\nAttention Flow Visualization:\n‚Ä¢ Title: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (90% attention)\n‚Ä¢ Abstract: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (75% attention)\n‚Ä¢ Introduction: ‚ñà‚ñà‚ñà‚ñà (45% attention)\n‚Ä¢ Methods: ‚ñà‚ñà (25% attention)\n‚Ä¢ Results: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (70% attention)\n‚Ä¢ Conclusion: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (85% attention)\n‚Ä¢ References: ‚ñà (10% attention)\n\nAttention Patterns Revealed:\n‚Ä¢ Strong focus on high-level content\n‚Ä¢ Skips detailed methodology\n‚Ä¢ Prioritizes conclusions over data\n‚Ä¢ May miss important nuances in methods\n\nInsights for Users:\n"The AI focused heavily on conclusions but gave limited attention to methodology. For technical accuracy, consider asking specifically about methods."\n\nModel Improvement:\n‚Ä¢ Rebalance attention weights for technical documents\n‚Ä¢ Add methodology-specific attention heads'
  },
  {
    id: 'uncertainty-quantification',
    name: 'Uncertainty Quantification',
    abbr: 'UQ',
    icon: 'üìä',
    color: 'from-purple-500 to-pink-600',
    category: 'interpretability',
    description: 'Quantify and communicate AI confidence and uncertainty in predictions',
    features: [
      'Confidence interval estimation',
      'Epistemic vs aleatoric uncertainty',
      'Prediction interval visualization',
      'Calibrated probability outputs',
      'Uncertainty propagation tracking',
      'Risk-adjusted recommendations'
    ],
    useCases: ['high-stakes-decisions', 'risk-management', 'scientific-modeling', 'financial-predictions'],
    complexity: 'high',
    example: 'Stock Price Prediction:\n\nPrediction: "AAPL will be $185 next week"\n\nUncertainty Breakdown:\n‚Ä¢ Point Prediction: $185.00\n‚Ä¢ Confidence Interval: $175.50 - $194.50 (90%)\n‚Ä¢ Prediction Interval: $170.00 - $200.00 (95%)\n\nUncertainty Sources:\n‚Ä¢ Market Volatility: ¬±$8.50 (aleatoric)\n‚Ä¢ Model Uncertainty: ¬±$6.25 (epistemic)\n‚Ä¢ Data Quality: ¬±$3.75 (epistemic)\n‚Ä¢ External Events: ¬±$15.00 (aleatoric)\n\nRisk Assessment:\n‚Ä¢ High Confidence Scenarios (60%): Range $180-$190\n‚Ä¢ Medium Confidence Scenarios (30%): Range $175-$195\n‚Ä¢ Low Confidence Scenarios (10%): Range $170-$200\n\nRecommendation:\n"Moderate confidence in upward trend, but significant uncertainty due to earnings announcement. Consider position sizing accordingly."\n\nUser Benefits:\n‚Ä¢ Understand prediction reliability\n‚Ä¢ Make risk-appropriate decisions\n‚Ä¢ Identify when to seek additional data'
  }
]; 