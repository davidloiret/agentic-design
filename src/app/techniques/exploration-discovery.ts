import { Technique } from './types';

export const explorationDiscoveryTechniques: Technique[] = [
  {
    id: 'reinforcement-learning',
    name: 'Reinforcement Learning Exploration',
    abbr: 'RLE',
    icon: 'ðŸŽ®',
    color: 'from-blue-500 to-indigo-600',
    category: 'exploration-discovery',
    description: 'Learning optimal behavior through reward-based exploration and exploitation balance',
    features: [
      'Reward function optimization',
      'Exploration-exploitation balance',
      'Policy gradient methods',
      'Q-learning variants',
      'Multi-armed bandit solutions',
      'Continuous learning adaptation'
    ],
    useCases: ['recommendation-systems', 'automated-trading', 'game-playing', 'resource-optimization'],
    complexity: 'high',
    example: 'AI Content Recommendation System:\n\nEnvironment Setup:\nâ€¢ State: User profile, content library, engagement history\nâ€¢ Actions: Recommend content from 10,000+ items\nâ€¢ Reward: User engagement (clicks, time spent, ratings)\nâ€¢ Goal: Maximize long-term user satisfaction\n\nExploration Strategy:\n\n1. Epsilon-Greedy Approach:\n   â€¢ 80% Exploitation: Recommend known good content\n   â€¢ 20% Exploration: Try new/different content\n   â€¢ Adaptive epsilon: Reduce exploration over time\n   â€¢ User-specific: New users get more exploration\n\n2. Upper Confidence Bound (UCB):\n   â€¢ Balance mean reward with uncertainty\n   â€¢ Formula: Î¼ + câˆš(ln(t)/n)\n   â€¢ Higher confidence â†’ More exploration\n   â€¢ Proven content â†’ Stable recommendations\n\n3. Thompson Sampling:\n   â€¢ Bayesian approach to exploration\n   â€¢ Sample from posterior distributions\n   â€¢ Natural exploration without hyperparameters\n   â€¢ Better for sparse reward scenarios\n\nLearning Process:\n\nWeek 1 (Cold Start):\nâ€¢ High exploration (Îµ=0.4)\nâ€¢ Random content sampling\nâ€¢ Building user preference model\nâ€¢ Engagement rate: 12%\n\nWeek 4 (Learning Phase):\nâ€¢ Moderate exploration (Îµ=0.2)\nâ€¢ Pattern recognition emerging\nâ€¢ Personalization improving\nâ€¢ Engagement rate: 28%\n\nWeek 12 (Optimization Phase):\nâ€¢ Low exploration (Îµ=0.1)\nâ€¢ Fine-tuned recommendations\nâ€¢ Occasional novelty injection\nâ€¢ Engagement rate: 45%\n\nAdvanced Techniques:\n\n1. Contextual Bandits:\n   â€¢ Context: Time, device, location, mood\n   â€¢ Action: Content recommendation\n   â€¢ Reward: Contextual engagement\n   â€¢ Learning: Context-aware policies\n\n2. Multi-Objective Optimization:\n   â€¢ Primary: User engagement\n   â€¢ Secondary: Content diversity\n   â€¢ Tertiary: Business objectives\n   â€¢ Balance: Pareto-optimal solutions\n\nResults:\nâ€¢ User engagement: +340% vs random\nâ€¢ Content discovery: +78% of catalog explored\nâ€¢ User retention: +56% long-term\nâ€¢ Revenue impact: +$2.8M annually'
  },
  {
    id: 'curiosity-driven-search',
    name: 'Curiosity-Driven Exploration',
    abbr: 'CDE',
    icon: 'ðŸ”',
    color: 'from-indigo-500 to-purple-600',
    category: 'exploration-discovery',
    description: 'Intrinsic motivation-based exploration that drives discovery of novel and surprising information',
    features: [
      'Intrinsic motivation modeling',
      'Novelty detection',
      'Surprise quantification',
      'Information gain maximization',
      'Uncertainty-based exploration',
      'Meta-learning capabilities'
    ],
    useCases: ['research-discovery', 'creative-exploration', 'scientific-discovery', 'knowledge-expansion'],
    complexity: 'high',
    example: 'Scientific Research Discovery Agent:\n\nCuriosity Framework:\n\n1. Information-Theoretic Curiosity:\n   â€¢ Measure: Mutual information between observations\n   â€¢ Drive: Seek observations that reduce uncertainty\n   â€¢ Example: "Why do these proteins fold differently?"\n   â€¢ Action: Design experiments to test hypotheses\n\n2. Prediction Error Curiosity:\n   â€¢ Measure: Difference between predicted and actual\n   â€¢ Drive: Investigate unexpected results\n   â€¢ Example: "Model predicted 80% accuracy, got 45%"\n   â€¢ Action: Analyze failure cases for insights\n\n3. Novelty-Based Curiosity:\n   â€¢ Measure: Distance from previously seen patterns\n   â€¢ Drive: Explore uncharted territories\n   â€¢ Example: "This molecular structure is unique"\n   â€¢ Action: Investigate properties and applications\n\nExploration Process:\n\nPhase 1: Hypothesis Generation\nâ€¢ Current knowledge: Protein folding patterns\nâ€¢ Curiosity trigger: Unusual folding in extreme temperatures\nâ€¢ Questions generated: 15 testable hypotheses\nâ€¢ Priority ranking: Information gain potential\n\nPhase 2: Experiment Design\nâ€¢ Most curious question: "Heat shock protein behavior"\nâ€¢ Experiment design: Temperature gradient analysis\nâ€¢ Expected information gain: 2.3 bits\nâ€¢ Resource cost: $12,000, 3 weeks\n\nPhase 3: Discovery Process\nâ€¢ Unexpected result: Protein maintains stability\nâ€¢ Surprise score: 8.7/10 (very unexpected)\nâ€¢ New hypothesis: Novel stabilization mechanism\nâ€¢ Follow-up curiosity: "What enables this stability?"\n\nPhase 4: Knowledge Integration\nâ€¢ Discovery: New class of thermophilic proteins\nâ€¢ Applications: Heat-resistant enzymes\nâ€¢ Publication: 3 papers, 47 citations\nâ€¢ Knowledge expansion: 23% increase in protein database\n\nCuriosity Metrics:\n\n1. Exploration Coverage:\n   â€¢ Research space explored: 34% increase\n   â€¢ Novel combinations tested: 156\n   â€¢ Unexpected discoveries: 12\n   â€¢ Knowledge gaps identified: 28\n\n2. Discovery Impact:\n   â€¢ Citation impact: +145% vs traditional research\n   â€¢ Novel insights: 8 breakthrough discoveries\n   â€¢ Patent applications: 5 filed\n   â€¢ Commercial applications: 3 developed\n\nAdaptive Curiosity:\nâ€¢ Learning: Successful curiosity patterns\nâ€¢ Adaptation: Focus on high-impact areas\nâ€¢ Meta-learning: Better question generation\nâ€¢ Collaboration: Share curiosity with other agents\n\nResults:\nâ€¢ Research efficiency: 3x faster discovery\nâ€¢ Innovation rate: +280% novel findings\nâ€¢ Interdisciplinary connections: +150%\nâ€¢ Scientific impact: Top 5% cited papers'
  },
  {
    id: 'multi-armed-bandits',
    name: 'Multi-Armed Bandit Optimization',
    abbr: 'MAB',
    icon: 'ðŸŽ°',
    color: 'from-purple-500 to-pink-600',
    category: 'exploration-discovery',
    description: 'Sequential decision-making under uncertainty with optimal exploration-exploitation trade-offs',
    features: [
      'Regret minimization',
      'Confidence bounds',
      'Bayesian optimization',
      'Contextual awareness',
      'Non-stationary adaptation',
      'Multi-objective handling'
    ],
    useCases: ['a-b-testing', 'ad-optimization', 'clinical-trials', 'resource-allocation'],
    complexity: 'medium',
    example: 'Dynamic Pricing Optimization:\n\nBandit Setup:\nâ€¢ Arms: 8 different pricing strategies\nâ€¢ Context: Customer segment, time, demand\nâ€¢ Reward: Revenue per sale\nâ€¢ Goal: Maximize total revenue\nâ€¢ Constraint: Maintain customer satisfaction\n\nPricing Arms:\n1. Premium (+20%): $120\n2. Standard (baseline): $100\n3. Value (-10%): $90\n4. Discount (-20%): $80\n5. Dynamic (demand-based): $85-115\n6. Psychological ($99, $89): Variable\n7. Bundle offers: $180 for 2\n8. Subscription: $8/month\n\nBandit Algorithm: Contextual Thompson Sampling\n\nWeek 1 Results:\nâ€¢ Exploration phase: Equal arm selection\nâ€¢ Premium: 15% conversion, $18/customer\nâ€¢ Standard: 25% conversion, $25/customer\nâ€¢ Value: 35% conversion, $31.50/customer\nâ€¢ Early leader: Value pricing\n\nWeek 4 Results:\nâ€¢ Exploitation increasing\nâ€¢ Context awareness: Segments respond differently\nâ€¢ Enterprise customers: Premium performs best\nâ€¢ Price-sensitive: Value pricing optimal\nâ€¢ Impulse buyers: Psychological pricing wins\n\nContextual Insights:\n\n1. Customer Segments:\n   â€¢ Enterprise (20%): Premium strategy (+35% revenue)\n   â€¢ SMB (40%): Standard strategy (baseline)\n   â€¢ Individual (30%): Value strategy (+18% revenue)\n   â€¢ Students (10%): Discount strategy (+12% volume)\n\n2. Temporal Patterns:\n   â€¢ Monday-Wednesday: Higher price tolerance\n   â€¢ Thursday-Friday: Standard pricing optimal\n   â€¢ Weekends: Promotional pricing effective\n   â€¢ Month-end: Bundle offers convert better\n\n3. Demand-Based Adaptation:\n   â€¢ High demand: Premium pricing (+25% revenue)\n   â€¢ Normal demand: Standard pricing\n   â€¢ Low demand: Value pricing maintains volume\n   â€¢ Inventory clearing: Discount pricing\n\nAdvanced Features:\n\n1. Non-Stationary Environment:\n   â€¢ Market conditions change\n   â€¢ Competitor pricing adjustments\n   â€¢ Seasonal demand variations\n   â€¢ Algorithm adapts arm probabilities\n\n2. Multi-Objective Optimization:\n   â€¢ Primary: Revenue maximization\n   â€¢ Secondary: Customer satisfaction >4.0\n   â€¢ Tertiary: Market share growth\n   â€¢ Constraint: Legal pricing bounds\n\n3. Risk-Aware Exploration:\n   â€¢ Conservative exploration for key customers\n   â€¢ Aggressive exploration for new segments\n   â€¢ Gradual price changes to avoid shock\n   â€¢ Rollback mechanisms for poor performance\n\nResults (6 months):\nâ€¢ Revenue increase: +23% vs fixed pricing\nâ€¢ Customer satisfaction: 4.2/5.0 (maintained)\nâ€¢ Market share: +8% growth\nâ€¢ Pricing efficiency: 94% optimal decisions'
  },
  {
    id: 'evolutionary-algorithms',
    name: 'Evolutionary Discovery Algorithms',
    abbr: 'EDA',
    icon: 'ðŸ§¬',
    color: 'from-pink-500 to-red-600',
    category: 'exploration-discovery',
    description: 'Bio-inspired optimization that evolves solutions through selection, mutation, and crossover',
    features: [
      'Population-based search',
      'Genetic operators',
      'Fitness evaluation',
      'Diversity maintenance',
      'Multi-objective evolution',
      'Adaptive parameter control'
    ],
    useCases: ['neural-architecture-search', 'hyperparameter-optimization', 'creative-design', 'scheduling-optimization'],
    complexity: 'high',
    example: 'AI Model Architecture Evolution:\n\nEvolution Setup:\nâ€¢ Objective: Find optimal neural network architecture\nâ€¢ Population: 50 different architectures\nâ€¢ Generations: 100 iterations\nâ€¢ Fitness: Accuracy - 0.1Ã—(Parameters/1M)\nâ€¢ Constraints: <10M parameters, <100ms inference\n\nGenetic Representation:\nâ€¢ Chromosome: [layers, neurons, activation, dropout]\nâ€¢ Layer types: Dense, Conv2D, LSTM, Attention\nâ€¢ Neuron counts: 16, 32, 64, 128, 256, 512\nâ€¢ Activations: ReLU, Tanh, Swish, GELU\nâ€¢ Dropout rates: 0.0, 0.1, 0.2, 0.3, 0.5\n\nGeneration 1 (Random Population):\nâ€¢ Best fitness: 0.73 (78% accuracy, 5.2M params)\nâ€¢ Average fitness: 0.45\nâ€¢ Diversity: High (all random architectures)\nâ€¢ Worst: 0.12 (overly complex, poor performance)\n\nGenetic Operations:\n\n1. Selection (Tournament, k=3):\n   â€¢ Select top 30% for reproduction\n   â€¢ Elite preservation: Keep top 5 unchanged\n   â€¢ Diversity bonus: Reward unique architectures\n\n2. Crossover (Uniform, p=0.8):\n   â€¢ Parent A: [4 layers, 128 neurons, ReLU, 0.2]\n   â€¢ Parent B: [6 layers, 64 neurons, Swish, 0.1]\n   â€¢ Child: [4 layers, 64 neurons, Swish, 0.2]\n\n3. Mutation (Gaussian, p=0.1):\n   â€¢ Layer count: Â±1 with 10% probability\n   â€¢ Neuron count: Adjacent size with 15% probability\n   â€¢ Activation: Random change with 5% probability\n   â€¢ Dropout: Â±0.1 with 10% probability\n\nGeneration 25 Results:\nâ€¢ Best fitness: 0.89 (92% accuracy, 3.1M params)\nâ€¢ Architecture: [5 layers, Conv2D+Attention+Dense]\nâ€¢ Innovation: Attention mechanism emerged\nâ€¢ Population convergence: 70% similar structures\n\nGeneration 50 Results:\nâ€¢ Best fitness: 0.94 (96% accuracy, 2.8M params)\nâ€¢ Architecture: Hybrid CNN-Transformer\nâ€¢ Breakthrough: Skip connections discovered\nâ€¢ Diversity maintenance: Introduced new mutations\n\nGeneration 100 (Final):\nâ€¢ Best fitness: 0.97 (98.2% accuracy, 2.3M params)\nâ€¢ Architecture: Optimized CNN-Transformer hybrid\nâ€¢ Performance: 94ms inference time\nâ€¢ Validation: Beats hand-designed architectures\n\nEvolutionary Insights:\n\n1. Emergent Patterns:\n   â€¢ Skip connections: Appeared in 85% of top solutions\n   â€¢ Attention mechanisms: Critical for performance\n   â€¢ Batch normalization: Universal adoption\n   â€¢ Optimal depth: 5-7 layers for this problem\n\n2. Trade-off Discovery:\n   â€¢ Accuracy vs Speed: Pareto frontier identified\n   â€¢ Complexity vs Generalization: Sweet spot found\n   â€¢ Memory vs Performance: Optimal configurations\n\n3. Unexpected Solutions:\n   â€¢ Asymmetric architectures outperformed symmetric\n   â€¢ Mixed precision training reduced parameters\n   â€¢ Ensemble emergence: Population diversity benefits\n\nResults:\nâ€¢ Architecture search time: 3 days vs 6 months manual\nâ€¢ Performance improvement: +15% vs best human design\nâ€¢ Parameter efficiency: 60% fewer parameters\nâ€¢ Novel architectural patterns: 3 publishable innovations'
  }
];