import { Technique } from './types';

export const prioritizationTechniques: Technique[] = [
  {
    id: 'weighted-scoring',
    name: 'Multi-Criteria Weighted Scoring',
    abbr: 'MCWS',
    icon: '‚öñÔ∏è',
    color: 'from-blue-500 to-indigo-600',
    category: 'prioritization',
    description: 'Systematic prioritization using weighted scoring across multiple evaluation criteria',
    features: [
      'Multi-criteria evaluation',
      'Weight assignment optimization',
      'Stakeholder preference integration',
      'Sensitivity analysis',
      'Trade-off visualization',
      'Decision transparency'
    ],
    useCases: ['feature-prioritization', 'project-selection', 'resource-allocation', 'vendor-selection'],
    complexity: 'medium',
    example: 'Product Feature Prioritization:\n\nEvaluation Criteria:\n1. User Impact (Weight: 30%)\n   ‚Ä¢ Affects how many users?\n   ‚Ä¢ Improves user experience how much?\n   ‚Ä¢ Reduces user friction?\n\n2. Business Value (Weight: 25%)\n   ‚Ä¢ Revenue potential\n   ‚Ä¢ Cost savings\n   ‚Ä¢ Strategic alignment\n\n3. Technical Feasibility (Weight: 20%)\n   ‚Ä¢ Development complexity\n   ‚Ä¢ Technical risk\n   ‚Ä¢ Resource requirements\n\n4. Time to Market (Weight: 15%)\n   ‚Ä¢ Development time\n   ‚Ä¢ Testing requirements\n   ‚Ä¢ Deployment complexity\n\n5. Competitive Advantage (Weight: 10%)\n   ‚Ä¢ Differentiation potential\n   ‚Ä¢ Market positioning\n   ‚Ä¢ Patent opportunities\n\nFeature Evaluation:\n\nFeature A: "AI-Powered Search"\n‚Ä¢ User Impact: 9/10 (affects all users, major UX improvement)\n‚Ä¢ Business Value: 8/10 (20% conversion increase projected)\n‚Ä¢ Technical Feasibility: 6/10 (requires ML expertise)\n‚Ä¢ Time to Market: 4/10 (6-month development)\n‚Ä¢ Competitive Advantage: 9/10 (unique in market)\n‚Ä¢ Weighted Score: (9√ó0.3)+(8√ó0.25)+(6√ó0.2)+(4√ó0.15)+(9√ó0.1) = 7.4\n\nFeature B: "Mobile App Notifications"\n‚Ä¢ User Impact: 7/10 (improves engagement)\n‚Ä¢ Business Value: 6/10 (moderate retention impact)\n‚Ä¢ Technical Feasibility: 9/10 (straightforward implementation)\n‚Ä¢ Time to Market: 9/10 (2-week development)\n‚Ä¢ Competitive Advantage: 4/10 (table stakes feature)\n‚Ä¢ Weighted Score: (7√ó0.3)+(6√ó0.25)+(9√ó0.2)+(9√ó0.15)+(4√ó0.1) = 7.0\n\nFeature C: "Advanced Analytics Dashboard"\n‚Ä¢ User Impact: 5/10 (power users only)\n‚Ä¢ Business Value: 7/10 (enterprise sales enabler)\n‚Ä¢ Technical Feasibility: 7/10 (moderate complexity)\n‚Ä¢ Time to Market: 6/10 (3-month development)\n‚Ä¢ Competitive Advantage: 6/10 (expected feature)\n‚Ä¢ Weighted Score: (5√ó0.3)+(7√ó0.25)+(7√ó0.2)+(6√ó0.15)+(6√ó0.1) = 6.0\n\nPrioritization Results:\n1. AI-Powered Search (7.4) - High Priority\n2. Mobile App Notifications (7.0) - Medium-High Priority\n3. Advanced Analytics Dashboard (6.0) - Medium Priority\n\nSensitivity Analysis:\n‚Ä¢ If Technical Feasibility weight increased to 30%:\n  - Feature B becomes #1 (higher feasibility)\n‚Ä¢ If Time to Market weight increased to 25%:\n  - Feature B significantly ahead\n‚Ä¢ Decision robustness: Feature A wins in 78% of weight scenarios\n\nStakeholder Alignment:\n‚Ä¢ Engineering team prefers Feature B (feasibility)\n‚Ä¢ Product team prefers Feature A (impact)\n‚Ä¢ Business team values Feature A (revenue potential)\n‚Ä¢ Final decision: Proceed with Feature A, quick-win Feature B next'
  },
  {
    id: 'multi-criteria-decision',
    name: 'Multi-Criteria Decision Analysis',
    abbr: 'MCDA',
    icon: 'üéØ',
    color: 'from-indigo-500 to-purple-600',
    category: 'prioritization',
    description: 'Advanced decision-making framework for complex choices with multiple competing objectives',
    features: [
      'Analytical Hierarchy Process',
      'TOPSIS methodology',
      'Fuzzy logic integration',
      'Pairwise comparisons',
      'Consistency checking',
      'Group decision support'
    ],
    useCases: ['strategic-planning', 'investment-decisions', 'technology-selection', 'policy-making'],
    complexity: 'high',
    example: 'Cloud Provider Selection:\n\nDecision Problem:\n‚Ä¢ Need to select cloud provider for enterprise AI workloads\n‚Ä¢ 3 alternatives: AWS, Azure, Google Cloud\n‚Ä¢ Multiple stakeholders: IT, Finance, Data Science, Security\n‚Ä¢ Budget: $2M annually, 5-year commitment\n\nCriteria Hierarchy:\n\nLevel 1: Main Criteria\n1. Technical Capabilities (40%)\n2. Cost Structure (25%)\n3. Security & Compliance (20%)\n4. Support & Reliability (15%)\n\nLevel 2: Sub-Criteria\nTechnical Capabilities:\n‚Ä¢ AI/ML Services (50%)\n‚Ä¢ Computing Power (30%)\n‚Ä¢ Integration Capabilities (20%)\n\nCost Structure:\n‚Ä¢ Base Pricing (60%)\n‚Ä¢ Scaling Costs (40%)\n\nSecurity & Compliance:\n‚Ä¢ Data Protection (70%)\n‚Ä¢ Compliance Certifications (30%)\n\nSupport & Reliability:\n‚Ä¢ SLA Guarantees (60%)\n‚Ä¢ Technical Support (40%)\n\nPairwise Comparisons:\n\nTechnical Capabilities (AI/ML Services):\n‚Ä¢ AWS vs Azure: AWS slightly better (3:1)\n‚Ä¢ AWS vs GCP: AWS moderately better (5:1)\n‚Ä¢ Azure vs GCP: Azure slightly better (3:1)\n‚Ä¢ Consistency ratio: 0.08 (acceptable < 0.1)\n\nCost Structure Analysis:\n‚Ä¢ AWS: Mature pricing, complex tiers\n‚Ä¢ Azure: Competitive pricing, good enterprise deals\n‚Ä¢ GCP: Aggressive pricing, simple structure\n‚Ä¢ Ranking: GCP > Azure > AWS\n\nTOPSIS Analysis:\n\nIdeal Solution (Best values across criteria):\n‚Ä¢ Technical: AWS score (9.2/10)\n‚Ä¢ Cost: GCP score (8.8/10)\n‚Ä¢ Security: Azure score (9.0/10)\n‚Ä¢ Support: AWS score (9.1/10)\n\nNegative Ideal (Worst values):\n‚Ä¢ Technical: 6.5/10\n‚Ä¢ Cost: 6.2/10\n‚Ä¢ Security: 7.1/10\n‚Ä¢ Support: 7.3/10\n\nDistance Calculations:\nAWS:\n‚Ä¢ Distance to ideal: 0.234\n‚Ä¢ Distance to negative ideal: 0.756\n‚Ä¢ Relative closeness: 0.763\n\nAzure:\n‚Ä¢ Distance to ideal: 0.312\n‚Ä¢ Distance to negative ideal: 0.642\n‚Ä¢ Relative closeness: 0.673\n\nGoogle Cloud:\n‚Ä¢ Distance to ideal: 0.445\n‚Ä¢ Distance to negative ideal: 0.498\n‚Ä¢ Relative closeness: 0.528\n\nFinal Rankings:\n1. AWS (0.763) - Best overall balance\n2. Azure (0.673) - Strong enterprise features\n3. Google Cloud (0.528) - Cost-effective but gaps\n\nSensitivity Analysis:\n‚Ä¢ If cost weight increased to 40%, Azure becomes #1\n‚Ä¢ If technical weight reduced to 25%, Azure competitive\n‚Ä¢ AWS wins in 82% of reasonable weight scenarios\n\nGroup Decision Integration:\n‚Ä¢ IT team: Prefers AWS (technical leadership)\n‚Ä¢ Finance team: Prefers GCP (cost savings)\n‚Ä¢ Security team: Prefers Azure (compliance features)\n‚Ä¢ Data Science team: Prefers AWS (ML tools)\n\nConsensus Building:\n‚Ä¢ Weighted stakeholder preferences\n‚Ä¢ Address key concerns of each group\n‚Ä¢ Negotiate hybrid or multi-cloud approach\n‚Ä¢ Final decision: AWS primary, Azure for specific compliance needs\n\nDecision Validation:\n‚Ä¢ ROI projection: 23% over 5 years with AWS\n‚Ä¢ Risk assessment: Low technical risk, medium vendor lock-in\n‚Ä¢ Implementation plan: 6-month migration timeline\n‚Ä¢ Success metrics: Performance, cost, user satisfaction'
  },
  {
    id: 'priority-queues',
    name: 'Dynamic Priority Queue Systems',
    abbr: 'DPQS',
    icon: 'üìä',
    color: 'from-purple-500 to-pink-600',
    category: 'prioritization',
    description: 'Intelligent queue management with dynamic priority adjustment based on changing conditions',
    features: [
      'Dynamic priority calculation',
      'Aging mechanisms',
      'SLA-aware scheduling',
      'Load balancing',
      'Fairness algorithms',
      'Real-time optimization'
    ],
    useCases: ['task-scheduling', 'customer-support', 'job-processing', 'resource-allocation'],
    complexity: 'medium',
    example: 'AI Model Training Queue:\n\nQueue System Overview:\n‚Ä¢ 500+ ML training jobs daily\n‚Ä¢ 50 GPU clusters (A100, V100, RTX series)\n‚Ä¢ Users: Research teams, product teams, students\n‚Ä¢ SLAs: Research (24h), Product (4h), Critical (1h)\n\nPriority Calculation Formula:\nPriority = (Base_Priority √ó User_Tier √ó Urgency) + Aging_Bonus - Resource_Cost_Penalty\n\nPriority Components:\n\n1. Base Priority (1-10):\n   ‚Ä¢ Critical production bugs: 10\n   ‚Ä¢ Product feature development: 7-8\n   ‚Ä¢ Research experiments: 4-6\n   ‚Ä¢ Student projects: 1-3\n\n2. User Tier Multiplier:\n   ‚Ä¢ Enterprise customers: 1.5x\n   ‚Ä¢ Internal product teams: 1.3x\n   ‚Ä¢ Research teams: 1.0x\n   ‚Ä¢ Students/free tier: 0.7x\n\n3. Urgency Factor (0.5-2.0):\n   ‚Ä¢ Emergency hotfix: 2.0x\n   ‚Ä¢ Deadline this week: 1.5x\n   ‚Ä¢ Deadline this month: 1.0x\n   ‚Ä¢ No specific deadline: 0.8x\n\n4. Aging Bonus:\n   ‚Ä¢ +0.1 per hour in queue\n   ‚Ä¢ Prevents starvation\n   ‚Ä¢ Caps at +2.0 after 20 hours\n\n5. Resource Cost Penalty:\n   ‚Ä¢ Large jobs (>8 GPUs): -1.0\n   ‚Ä¢ Long jobs (>24h): -0.5\n   ‚Ä¢ Encourages efficient resource use\n\nExample Queue State:\n\nJob A: Research experiment\n‚Ä¢ Base Priority: 5\n‚Ä¢ User Tier: 1.0x (research)\n‚Ä¢ Urgency: 1.0x (no urgent deadline)\n‚Ä¢ Age: 3 hours (+0.3)\n‚Ä¢ Resource Cost: 2 GPUs (-0.0)\n‚Ä¢ Final Priority: 5.3\n\nJob B: Production model update\n‚Ä¢ Base Priority: 8\n‚Ä¢ User Tier: 1.3x (product team)\n‚Ä¢ Urgency: 1.5x (deadline this week)\n‚Ä¢ Age: 0.5 hours (+0.05)\n‚Ä¢ Resource Cost: 16 GPUs (-1.0)\n‚Ä¢ Final Priority: (8 √ó 1.3 √ó 1.5) + 0.05 - 1.0 = 14.65\n\nJob C: Student thesis experiment\n‚Ä¢ Base Priority: 2\n‚Ä¢ User Tier: 0.7x (student)\n‚Ä¢ Urgency: 0.8x (no urgent deadline)\n‚Ä¢ Age: 12 hours (+1.2)\n‚Ä¢ Resource Cost: 1 GPU (-0.0)\n‚Ä¢ Final Priority: (2 √ó 0.7 √ó 0.8) + 1.2 = 2.32\n\nQueue Order: Job B (14.65) ‚Üí Job A (5.3) ‚Üí Job C (2.32)\n\nDynamic Adjustments:\n\nReal-time Priority Updates:\n‚Ä¢ Every 15 minutes: Recalculate all priorities\n‚Ä¢ SLA monitoring: Boost jobs approaching deadline\n‚Ä¢ Resource availability: Adjust for cluster status\n‚Ä¢ Fair share: Prevent user monopolization\n\nSLA Management:\n‚Ä¢ Research SLA: 24 hours\n‚Ä¢ Job A at 20 hours: Priority boost +3.0\n‚Ä¢ Ensures SLA compliance: 99.2% achievement rate\n\nLoad Balancing:\n‚Ä¢ GPU utilization: 94% average\n‚Ä¢ Queue wait time: 2.3 hours average\n‚Ä¢ Fairness index: 0.87 (good distribution)\n‚Ä¢ Resource fragmentation: 12% (acceptable)\n\nAdaptive Scheduling:\n\n1. Backfill Scheduling:\n   ‚Ä¢ Small jobs fill gaps left by large jobs\n   ‚Ä¢ Improves overall throughput by 34%\n   ‚Ä¢ No impact on high-priority job SLAs\n\n2. Gang Scheduling:\n   ‚Ä¢ Multi-GPU jobs scheduled atomically\n   ‚Ä¢ Reduces resource fragmentation\n   ‚Ä¢ Improves large job completion rates\n\n3. Preemption Handling:\n   ‚Ä¢ Critical jobs can preempt lower priority\n   ‚Ä¢ Checkpointing for interrupted jobs\n   ‚Ä¢ Automatic restart with priority boost\n\nPerformance Metrics:\n\n‚Ä¢ Queue throughput: 523 jobs/day (+18% vs FIFO)\n‚Ä¢ Average wait time: 2.3h vs 4.1h (FIFO)\n‚Ä¢ SLA compliance: 99.2% vs 87% (FIFO)\n‚Ä¢ Resource utilization: 94% vs 78% (FIFO)\n‚Ä¢ User satisfaction: 8.7/10 vs 6.2/10 (FIFO)\n\nBusiness Impact:\n‚Ä¢ Research productivity: +45% experiments completed\n‚Ä¢ Product velocity: +67% faster model deployments\n‚Ä¢ Infrastructure ROI: +89% better GPU utilization\n‚Ä¢ Cost savings: $2.3M annually from efficiency gains'
  },
  {
    id: 'dynamic-ranking',
    name: 'Dynamic Content Ranking',
    abbr: 'DCR',
    icon: 'üèÜ',
    color: 'from-pink-500 to-red-600',
    category: 'prioritization',
    description: 'Real-time content and recommendation ranking that adapts to user behavior and context',
    features: [
      'Real-time personalization',
      'Context-aware ranking',
      'Multi-objective optimization',
      'A/B testing integration',
      'Feedback loop learning',
      'Exploration-exploitation balance'
    ],
    useCases: ['content-recommendation', 'search-ranking', 'feed-optimization', 'product-recommendations'],
    complexity: 'high',
    example: 'News Feed Ranking System:\n\nRanking Architecture:\n‚Ä¢ 10M+ articles processed daily\n‚Ä¢ 500K+ active users\n‚Ä¢ Real-time personalization\n‚Ä¢ Multi-objective optimization: Engagement + Diversity + Freshness\n\nRanking Features:\n\n1. Content Features (30% weight):\n   ‚Ä¢ Topic relevance to user interests\n   ‚Ä¢ Content quality score (readability, credibility)\n   ‚Ä¢ Viral potential (social sharing signals)\n   ‚Ä¢ Freshness (recency bias with decay)\n\n2. User Features (40% weight):\n   ‚Ä¢ Historical engagement patterns\n   ‚Ä¢ Topic preferences (learned)\n   ‚Ä¢ Reading time patterns\n   ‚Ä¢ Device and context (mobile vs desktop)\n\n3. Context Features (20% weight):\n   ‚Ä¢ Time of day (morning news vs evening entertainment)\n   ‚Ä¢ Day of week (weekend vs weekday preferences)\n   ‚Ä¢ Location (local news boost)\n   ‚Ä¢ Trending topics (social momentum)\n\n4. Interaction Features (10% weight):\n   ‚Ä¢ Similar users\' engagement\n   ‚Ä¢ Social network activity\n   ‚Ä¢ Comment and share predictions\n   ‚Ä¢ Cross-platform signals\n\nDynamic Ranking Process:\n\nStep 1: Candidate Generation\n‚Ä¢ Content pool: 50,000 articles\n‚Ä¢ User interest filtering: Reduce to 5,000\n‚Ä¢ Diversity sampling: Include 20% exploratory content\n‚Ä¢ Fresh content injection: Include 500 recent articles\n\nStep 2: Feature Extraction\n‚Ä¢ Content analysis: NLP processing, topic modeling\n‚Ä¢ User profiling: Real-time preference updates\n‚Ä¢ Context detection: Time, location, device signals\n‚Ä¢ Interaction signals: Social and behavioral data\n\nStep 3: Multi-Objective Scoring\n‚Ä¢ Engagement prediction: CTR, reading time, shares\n‚Ä¢ Diversity scoring: Topic and source variety\n‚Ä¢ Freshness weighting: Time-decay functions\n‚Ä¢ Quality assessment: Credibility and readability\n\nStep 4: Ranking Optimization\n‚Ä¢ Linear combination: Weighted sum of objectives\n‚Ä¢ Constraint satisfaction: Diversity quotas\n‚Ä¢ Exploration injection: 15% random exploration\n‚Ä¢ Personalization: User-specific weight adjustments\n\nReal-time Adaptation:\n\nUser Behavior Learning:\n‚Ä¢ Click-through: +0.1 to topic affinity\n‚Ä¢ Reading time >2min: +0.2 to content type preference\n‚Ä¢ Share/comment: +0.3 to topic and source preference\n‚Ä¢ Skip/back: -0.1 to similar content\n\nContextual Adjustments:\n‚Ä¢ Morning (6-10 AM): News and updates (+0.3)\n‚Ä¢ Lunch (12-1 PM): Quick reads and entertainment (+0.2)\n‚Ä¢ Evening (6-10 PM): Long-form and analysis (+0.4)\n‚Ä¢ Weekend: Lifestyle and leisure content (+0.5)\n\nTrending Integration:\n‚Ä¢ Viral content detection: Share velocity analysis\n‚Ä¢ Topic momentum: Search and social media signals\n‚Ä¢ Breaking news boost: Authority source amplification\n‚Ä¢ Seasonal adjustment: Holiday and event-driven content\n\nA/B Testing Integration:\n\nExperiment Framework:\n‚Ä¢ 5% traffic for ranking experiments\n‚Ä¢ Statistical significance testing\n‚Ä¢ Multi-arm bandit optimization\n‚Ä¢ Gradual rollout of winning variants\n\nCurrent Experiments:\n‚Ä¢ Freshness vs Relevance trade-off\n‚Ä¢ Diversity quota optimization (15% vs 25%)\n‚Ä¢ Personalization depth (individual vs cohort)\n‚Ä¢ Exploration rate tuning (10% vs 20%)\n\nPerformance Metrics:\n\nEngagement Metrics:\n‚Ä¢ Click-through rate: 8.7% (+23% vs baseline)\n‚Ä¢ Average reading time: 2.4 minutes (+34%)\n‚Ä¢ Shares per article: 0.31 (+67%)\n‚Ä¢ Return visits: 3.2 daily (+28%)\n\nDiversity Metrics:\n‚Ä¢ Topic coverage: 87% of user interests\n‚Ä¢ Source diversity: 45 unique publishers per user\n‚Ä¢ Viewpoint balance: 78% ideological diversity\n‚Ä¢ Serendipity rate: 22% unexpected discoveries\n\nBusiness Impact:\n‚Ä¢ User engagement: +45% time on platform\n‚Ä¢ Content consumption: +67% articles read\n‚Ä¢ User retention: +23% monthly active users\n‚Ä¢ Revenue: +$12M annually from improved engagement\n‚Ä¢ Publisher satisfaction: +34% content reach\n\nAdaptive Learning Results:\n‚Ä¢ Cold start problem: 67% improvement for new users\n‚Ä¢ Seasonal adaptation: 89% accuracy in preference shifts\n‚Ä¢ Real-time responsiveness: <100ms ranking updates\n‚Ä¢ Personalization effectiveness: 94% user satisfaction'
  }
];