import { Technique } from './types';

export const resourceAwareOptimizationTechniques: Technique[] = [
  {
    id: 'adaptive-compute-scaling',
    name: 'Adaptive Compute Scaling',
    abbr: 'ACS',
    icon: 'ðŸ“ˆ',
    color: 'from-green-500 to-emerald-600',
    category: 'resource-aware-optimization',
    description: 'Dynamically adjusts computational resources based on workload demands and performance requirements',
    features: [
      'Real-time resource monitoring',
      'Dynamic scaling decisions',
      'Performance-based adjustments',
      'Cost optimization',
      'Quality maintenance',
      'Predictive scaling'
    ],
    useCases: ['cloud-services', 'edge-computing', 'mobile-ai', 'cost-optimization'],
    complexity: 'high',
    example: 'AI Service Auto-Scaling:\n\nMonitoring:\nâ€¢ CPU Usage: 85% (threshold: 80%)\nâ€¢ Memory: 6.2GB/8GB (77%)\nâ€¢ Response Time: 1.2s (target: <1s)\nâ€¢ Queue Length: 45 requests\nâ€¢ Cost: $12/hour current\n\nDecision Engine:\nâ€¢ High CPU + slow response â†’ Scale up\nâ€¢ Queue growing â†’ Add 2 instances\nâ€¢ Cost constraint â†’ Use spot instances\nâ€¢ Peak hours ending â†’ Prepare scale down\n\nActions:\nâ€¢ Launch 2 additional instances\nâ€¢ Distribute load evenly\nâ€¢ Monitor for 5 minutes\nâ€¢ Result: 450ms response time, $18/hour\n\nPredictive Scaling:\nâ€¢ Historical pattern: +40% load at 2pm\nâ€¢ Pre-scale at 1:45pm\nâ€¢ Avoid performance degradation\nâ€¢ Save 30% on emergency scaling costs'
  },
  {
    id: 'cost-aware-model-selection',
    name: 'Cost-Aware Model Selection',
    abbr: 'CAMS',
    icon: 'ðŸ’°',
    color: 'from-emerald-500 to-teal-600',
    category: 'resource-aware-optimization',
    description: 'Intelligently selects AI models based on cost-performance trade-offs for specific tasks',
    features: [
      'Multi-model comparison',
      'Cost-performance analysis',
      'Dynamic model switching',
      'Quality thresholds',
      'Budget constraints',
      'Performance monitoring'
    ],
    useCases: ['api-optimization', 'budget-management', 'multi-model-systems', 'production-ai'],
    complexity: 'medium',
    example: 'Smart Model Router:\n\nTask: Text summarization\nBudget: $100/day\nQuality requirement: >85% accuracy\n\nModel Options:\nâ€¢ GPT-4: $0.03/1K tokens, 95% accuracy\nâ€¢ GPT-3.5: $0.002/1K tokens, 88% accuracy  \nâ€¢ Local model: $0.0001/1K tokens, 80% accuracy\n\nDecision Logic:\nâ€¢ High-priority requests â†’ GPT-4\nâ€¢ Standard requests â†’ GPT-3.5\nâ€¢ Bulk processing â†’ Local model\nâ€¢ Budget 80% used â†’ Switch to cheaper models\n\nReal-time Optimization:\nâ€¢ Monitor quality scores\nâ€¢ Track cost accumulation\nâ€¢ Adjust model selection\nâ€¢ Maintain SLA compliance\n\nResult: 90% cost reduction while maintaining 87% average quality'
  },
  {
    id: 'energy-efficient-inference',
    name: 'Energy-Efficient Inference',
    abbr: 'EEI',
    icon: 'ðŸ”‹',
    color: 'from-teal-500 to-cyan-600',
    category: 'resource-aware-optimization',
    description: 'Optimizes AI inference for minimal energy consumption while maintaining performance',
    features: [
      'Power consumption monitoring',
      'Dynamic frequency scaling',
      'Model compression',
      'Batch optimization',
      'Hardware acceleration',
      'Thermal management'
    ],
    useCases: ['mobile-devices', 'iot-systems', 'edge-ai', 'green-computing'],
    complexity: 'high',
    example: 'Mobile AI Optimization:\n\nDevice State:\nâ€¢ Battery: 25% remaining\nâ€¢ Temperature: 42Â°C\nâ€¢ CPU frequency: 2.4GHz\nâ€¢ Available RAM: 2.1GB\n\nEnergy Optimization:\nâ€¢ Reduce model precision: FP32 â†’ FP16\nâ€¢ Lower CPU frequency: 2.4GHz â†’ 1.8GHz\nâ€¢ Batch similar requests\nâ€¢ Use hardware accelerator (NPU)\nâ€¢ Cache frequent predictions\n\nTechniques:\nâ€¢ Quantization: 4x smaller models\nâ€¢ Pruning: Remove 60% of parameters\nâ€¢ Knowledge distillation: Teacher-student models\nâ€¢ Dynamic inference: Skip layers for simple tasks\n\nResults:\nâ€¢ 70% reduction in energy consumption\nâ€¢ 3x longer battery life\nâ€¢ 10Â°C lower operating temperature\nâ€¢ Maintained 95% of original accuracy'
  },
  {
    id: 'memory-optimization',
    name: 'Memory Optimization',
    abbr: 'MO',
    icon: 'ðŸ§ ',
    color: 'from-cyan-500 to-blue-600',
    category: 'resource-aware-optimization',
    description: 'Efficiently manages memory usage through caching, compression, and garbage collection strategies',
    features: [
      'Memory pool management',
      'Intelligent caching',
      'Compression algorithms',
      'Garbage collection tuning',
      'Memory-mapped files',
      'Stream processing'
    ],
    useCases: ['large-models', 'streaming-data', 'embedded-systems', 'high-throughput'],
    complexity: 'high',
    example: 'Large Language Model Optimization:\n\nMemory Challenges:\nâ€¢ Model size: 175B parameters (350GB)\nâ€¢ Available RAM: 80GB\nâ€¢ Concurrent users: 1000+\nâ€¢ Real-time inference required\n\nOptimization Strategies:\n\n1. Model Sharding:\n   â€¢ Split across 8 GPUs\n   â€¢ Pipeline parallel processing\n   â€¢ Minimize inter-GPU communication\n\n2. Dynamic Loading:\n   â€¢ Load model layers on-demand\n   â€¢ Keep hot layers in memory\n   â€¢ Swap cold layers to SSD\n\n3. KV-Cache Optimization:\n   â€¢ Compress attention matrices\n   â€¢ Use block-sparse attention\n   â€¢ Implement sliding window\n\n4. Memory Pool:\n   â€¢ Pre-allocate common tensor sizes\n   â€¢ Reuse memory buffers\n   â€¢ Minimize fragmentation\n\nResults:\nâ€¢ 80% memory usage reduction\nâ€¢ 5x improvement in throughput\nâ€¢ 50ms latency maintained\nâ€¢ Support 10x more concurrent users'
  },
  {
    id: 'latency-optimization',
    name: 'Latency Optimization',
    abbr: 'LO',
    icon: 'âš¡',
    color: 'from-blue-500 to-indigo-600',
    category: 'resource-aware-optimization',
    description: 'Minimizes response time through predictive loading, caching, and request optimization',
    features: [
      'Predictive prefetching',
      'Response caching',
      'Request batching',
      'Asynchronous processing',
      'Edge deployment',
      'Connection pooling'
    ],
    useCases: ['real-time-systems', 'interactive-ai', 'gaming', 'live-streaming'],
    complexity: 'medium',
    example: 'Real-time AI Assistant:\n\nLatency Requirements:\nâ€¢ Voice response: <200ms\nâ€¢ Text processing: <50ms\nâ€¢ Image analysis: <500ms\nâ€¢ User satisfaction: >95%\n\nOptimization Techniques:\n\n1. Predictive Loading:\n   â€¢ Analyze user patterns\n   â€¢ Pre-load likely models\n   â€¢ Warm up frequently used endpoints\n\n2. Intelligent Caching:\n   â€¢ Cache common responses\n   â€¢ Use semantic similarity\n   â€¢ Implement LRU eviction\n\n3. Request Optimization:\n   â€¢ Batch similar requests\n   â€¢ Parallel processing\n   â€¢ Stream partial results\n\n4. Infrastructure:\n   â€¢ Edge deployment (50ms closer)\n   â€¢ CDN for static content\n   â€¢ Dedicated GPU pools\n\nResults:\nâ€¢ Voice latency: 180ms average\nâ€¢ Text processing: 35ms average\nâ€¢ Cache hit rate: 78%\nâ€¢ User satisfaction: 97%\nâ€¢ 60% reduction in compute costs'
  }
];