import { Technique } from './types';

export const evaluationMonitoringTechniques: Technique[] = [
  {
    id: 'metrics-dashboards',
    name: 'AI Metrics Dashboards',
    abbr: 'AMD',
    icon: 'ğŸ“Š',
    color: 'from-blue-500 to-indigo-600',
    category: 'evaluation-monitoring',
    description: 'Comprehensive real-time monitoring dashboards for AI system performance, quality, and operational metrics',
    features: [
      'Real-time performance tracking',
      'Multi-dimensional metrics',
      'Custom alerting rules',
      'Trend analysis',
      'Comparative benchmarking',
      'Drill-down capabilities'
    ],
    useCases: ['production-monitoring', 'performance-optimization', 'sla-tracking', 'quality-assurance'],
    complexity: 'medium',
    example: 'Production AI Service Dashboard:\n\nCore Performance Metrics:\nâ€¢ Latency: P50: 120ms, P95: 450ms, P99: 890ms\nâ€¢ Throughput: 2,450 requests/minute\nâ€¢ Success Rate: 99.7% (SLA: 99.5%)\nâ€¢ Error Rate: 0.3% (mostly timeout errors)\n\nQuality Metrics:\nâ€¢ Accuracy Score: 94.2% (trending up 2.1%)\nâ€¢ User Satisfaction: 4.6/5.0 (based on feedback)\nâ€¢ Content Safety: 99.9% (automated filtering)\nâ€¢ Hallucination Rate: 0.8% (manual evaluation)\n\nResource Utilization:\nâ€¢ CPU Usage: 76% average, 89% peak\nâ€¢ Memory: 12.4GB/16GB (77% utilization)\nâ€¢ GPU Usage: 82% (3x NVIDIA A100)\nâ€¢ Cost: $1,247/day ($0.034/request)\n\nBusiness Metrics:\nâ€¢ Active Users: 45,230 (24hr)\nâ€¢ Conversion Rate: 12.4% (+0.8% vs last week)\nâ€¢ Revenue Impact: +$23,450/day from AI features\nâ€¢ Customer Retention: 96.2% (AI-assisted users)\n\nAlert Conditions:\nğŸŸ¡ Warning: P95 latency > 400ms for 5 minutes\nğŸ”´ Critical: Success rate < 99% for 10 minutes\nğŸŸ¢ All systems normal\n\nTrending Insights:\nâ€¢ Performance improved 15% after model optimization\nâ€¢ Quality scores correlate with user engagement\nâ€¢ Peak usage: 2-4pm ET, scale accordingly\nâ€¢ Cost per request down 23% vs last month'
  },
  {
    id: 'automated-testing',
    name: 'Automated AI Testing',
    abbr: 'AAT',
    icon: 'ğŸ§ª',
    color: 'from-indigo-500 to-purple-600',
    category: 'evaluation-monitoring',
    description: 'Comprehensive automated testing frameworks for AI model behavior, safety, and performance validation',
    features: [
      'Regression testing',
      'Performance benchmarking',
      'Safety evaluation',
      'Edge case detection',
      'A/B testing automation',
      'Continuous validation'
    ],
    useCases: ['model-validation', 'deployment-safety', 'regression-detection', 'quality-gates'],
    complexity: 'high',
    example: 'Automated LLM Testing Pipeline:\n\nTest Suite Categories:\n\n1. Functional Tests (2,450 test cases):\n   â€¢ Input-output validation\n   â€¢ Task completion accuracy\n   â€¢ Response format compliance\n   â€¢ Multi-language support\n   â€¢ Results: 98.4% pass rate\n\n2. Safety Tests (1,200 test cases):\n   â€¢ Harmful content detection\n   â€¢ Bias evaluation across demographics\n   â€¢ Privacy leak prevention\n   â€¢ Prompt injection resistance\n   â€¢ Results: 99.8% pass rate\n\n3. Performance Tests (800 test cases):\n   â€¢ Latency benchmarks\n   â€¢ Memory usage patterns\n   â€¢ Concurrent load handling\n   â€¢ Resource scaling behavior\n   â€¢ Results: All within SLA requirements\n\n4. Edge Case Tests (950 test cases):\n   â€¢ Unusual input formats\n   â€¢ Boundary conditions\n   â€¢ Error handling scenarios\n   â€¢ Recovery mechanisms\n   â€¢ Results: 94.2% handled gracefully\n\nAutomated Execution:\nâ€¢ Trigger: Every code commit + daily schedule\nâ€¢ Duration: 45 minutes full suite\nâ€¢ Environment: Isolated test infrastructure\nâ€¢ Reporting: Slack alerts + detailed reports\n\nRegression Detection:\nâ€¢ Baseline: Previous production model\nâ€¢ Comparison: Statistical significance testing\nâ€¢ Threshold: <2% accuracy degradation\nâ€¢ Action: Block deployment if regression detected\n\nContinuous Learning:\nâ€¢ Failed test analysis\nâ€¢ New test case generation\nâ€¢ Performance trend tracking\nâ€¢ Predictive failure detection\n\nResults Impact:\nâ€¢ 87% reduction in production bugs\nâ€¢ 95% faster deployment cycles\nâ€¢ Improved team confidence in releases\nâ€¢ $340K saved in prevented incidents'
  },
  {
    id: 'statistical-monitoring',
    name: 'Statistical Performance Monitoring',
    abbr: 'SPM',
    icon: 'ğŸ“ˆ',
    color: 'from-purple-500 to-pink-600',
    category: 'evaluation-monitoring',
    description: 'Advanced statistical analysis of AI system performance with drift detection and anomaly identification',
    features: [
      'Distribution monitoring',
      'Drift detection algorithms',
      'Anomaly identification',
      'Confidence intervals',
      'Hypothesis testing',
      'Predictive analytics'
    ],
    useCases: ['drift-detection', 'anomaly-detection', 'performance-analysis', 'quality-control'],
    complexity: 'high',
    example: 'Model Performance Drift Detection:\n\nStatistical Monitoring Framework:\n\n1. Baseline Establishment (30 days):\n   â€¢ Accuracy: Î¼=94.2%, Ïƒ=1.8%\n   â€¢ Latency: Î¼=145ms, Ïƒ=23ms\n   â€¢ User satisfaction: Î¼=4.4/5, Ïƒ=0.6\n   â€¢ Error patterns: Documented distribution\n\n2. Real-time Monitoring:\n   â€¢ Sample size: 1000 predictions/hour\n   â€¢ Statistical tests: Kolmogorov-Smirnov, Mann-Whitney\n   â€¢ Confidence level: 99% (Î±=0.01)\n   â€¢ Window: Sliding 24-hour periods\n\n3. Drift Detection Results:\n   Week 1: No significant drift detected\n   Week 2: âš ï¸ Accuracy drift detected\n   â€¢ Current: 91.8% (vs baseline 94.2%)\n   â€¢ P-value: 0.003 (significant)\n   â€¢ Effect size: -2.4% (moderate drift)\n   â€¢ Root cause: New data domain\n\n4. Anomaly Detection:\n   â€¢ Method: Isolation Forest + Statistical control charts\n   â€¢ Anomalies detected: 12 in last 7 days\n   â€¢ Pattern: Latency spikes during peak hours\n   â€¢ Action: Infrastructure scaling triggered\n\n5. Predictive Analysis:\n   â€¢ Trend: Gradual accuracy decline (-0.1%/week)\n   â€¢ Forecast: Will breach threshold in 8 weeks\n   â€¢ Recommendation: Schedule model retraining\n   â€¢ Confidence: 89% prediction accuracy\n\nAutomated Response:\nâ€¢ Minor drift: Enhanced monitoring\nâ€¢ Moderate drift: Alert ML team\nâ€¢ Severe drift: Automatic rollback\nâ€¢ Anomalies: Real-time notifications\n\nBusiness Impact:\nâ€¢ Early detection: 3 weeks before user impact\nâ€¢ Prevented outages: 5 incidents\nâ€¢ Cost savings: $125K in avoided downtime\nâ€¢ Improved reliability: 99.97% uptime'
  },
  {
    id: 'user-feedback-loops',
    name: 'User Feedback Integration',
    abbr: 'UFI',
    icon: 'ğŸ”„',
    color: 'from-pink-500 to-red-600',
    category: 'evaluation-monitoring',
    description: 'Systematic collection and integration of user feedback for continuous AI system improvement',
    features: [
      'Multi-channel feedback collection',
      'Sentiment analysis',
      'Feedback prioritization',
      'Automated response routing',
      'Improvement tracking',
      'User engagement metrics'
    ],
    useCases: ['user-experience', 'product-improvement', 'quality-enhancement', 'customer-satisfaction'],
    complexity: 'medium',
    example: 'AI Assistant Feedback Loop System:\n\nFeedback Collection Channels:\n\n1. Direct Rating System:\n   â€¢ Thumbs up/down on responses\n   â€¢ 5-star quality ratings\n   â€¢ Collection rate: 34% of interactions\n   â€¢ Average rating: 4.2/5.0\n\n2. Implicit Feedback:\n   â€¢ Response acceptance (copy/use): 78%\n   â€¢ Follow-up questions: 23%\n   â€¢ Session abandonment: 5.2%\n   â€¢ Retry requests: 8.7%\n\n3. Explicit Comments (2,450/month):\n   â€¢ Text feedback analysis\n   â€¢ Sentiment: 72% positive, 18% neutral, 10% negative\n   â€¢ Common themes: Speed (+), accuracy (+), understanding (-)\n\n4. Support Tickets (145/month):\n   â€¢ AI-related issues: 23%\n   â€¢ Response quality: 45%\n   â€¢ Feature requests: 32%\n\nFeedback Processing Pipeline:\n\n1. Real-time Collection:\n   â€¢ Capture all feedback types\n   â€¢ Metadata: user context, session info\n   â€¢ Privacy compliance: GDPR/CCPA\n\n2. Automated Analysis:\n   â€¢ Sentiment classification (96% accuracy)\n   â€¢ Topic modeling: 12 main categories\n   â€¢ Priority scoring: urgency Ã— impact\n   â€¢ Trend detection: weekly/monthly patterns\n\n3. Actionable Insights:\n   â€¢ Critical issues: Immediate escalation\n   â€¢ Feature gaps: Product backlog\n   â€¢ Quality patterns: Model improvement\n   â€¢ User education: Documentation updates\n\n4. Improvement Tracking:\n   â€¢ Response quality: +0.3 stars/month\n   â€¢ User satisfaction: 89% â†’ 94% (6 months)\n   â€¢ Feature adoption: +45% after feedback-driven changes\n   â€¢ Support tickets: -38% after improvements\n\nClosed-Loop Validation:\nâ€¢ A/B test improvements\nâ€¢ Measure feedback sentiment changes\nâ€¢ Track user behavior modifications\nâ€¢ Calculate ROI of feedback-driven changes\n\nResults:\nâ€¢ User satisfaction up 15%\nâ€¢ Product development 40% more targeted\nâ€¢ Support load reduced 30%\nâ€¢ Revenue impact: +$2.3M from improvements'
  }
];