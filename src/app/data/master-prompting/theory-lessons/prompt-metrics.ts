import { TheoryLesson } from '../../knowledge-representation/theory-lessons/types';

export const promptMetricsLesson: TheoryLesson = {
  id: 'prompt-metrics',
  title: 'Measuring Prompt Effectiveness: Metrics and Evaluation',
  description: 'Master the science of evaluating prompt performance through quantitative metrics and systematic testing methodologies',

  learningObjectives: [
    'Understand key metrics for evaluating prompt effectiveness',
    'Learn how to design and conduct prompt evaluation experiments',
    'Master statistical methods for comparing prompt variations',
    'Develop skills for building prompt evaluation pipelines',
    'Create comprehensive prompt performance dashboards'
  ],

  prerequisites: ['what-is-prompting', 'zero-shot-prompting'],

  sections: [
    {
      id: 'metrics-framework',
      title: 'Comprehensive Metrics Framework',
      content: `## The Science of Prompt Measurement

### Core Metric Categories

Effective prompt evaluation requires measuring multiple dimensions:

### 1. Task Performance Metrics

**Accuracy Metrics:**
- **Correctness Rate**: Percentage of factually correct outputs
- **Precision**: Relevant correct / Total generated
- **Recall**: Relevant correct / Total expected
- **F1 Score**: Harmonic mean of precision and recall

**Quality Metrics:**
- **Completeness**: All requirements addressed
- **Coherence**: Logical flow and consistency
- **Relevance**: On-topic and focused
- **Depth**: Level of detail and analysis

### 2. Efficiency Metrics

**Resource Consumption:**
- **Token Usage**: Input + output tokens
- **Latency**: Time to first token and completion
- **Cost per Query**: Dollar amount per prompt
- **Throughput**: Queries per second

**Token Efficiency Ratio (TER):**
\`TER = Quality Score / Token Count\`

Higher TER indicates better prompt efficiency.

### 3. Reliability Metrics

**Consistency Measures:**
- **Inter-run Agreement**: Similarity across multiple runs
- **Format Stability**: Consistent structure adherence
- **Variance Score**: Standard deviation of quality scores
- **Failure Rate**: Percentage of unusable outputs

### 4. Business Impact Metrics

**Value Metrics:**
- **Task Completion Rate**: Successfully finished tasks
- **User Satisfaction Score**: Human evaluation ratings
- **Time Saved**: Automation efficiency gains
- **Error Reduction**: Decrease in mistakes vs baseline

### Composite Scoring

Creating a holistic prompt score:

\`Prompt Score = w‚ÇÅ(Accuracy) + w‚ÇÇ(Efficiency) + w‚ÇÉ(Reliability) + w‚ÇÑ(Impact)\`

Where weights (w) reflect business priorities.`,
      examples: [
        {
          title: 'Comprehensive Metrics Dashboard',
          code: `# Prompt Evaluation Report
## Prompt: Customer Service Email Responder v2.3

### Performance Metrics
- Accuracy: 92% (target: 90%)
- Completeness: 95% (all issues addressed)
- Relevance: 98% (minimal off-topic content)
- Tone Match: 87% (professional, empathetic)

### Efficiency Metrics
- Avg Tokens Used: 245 (input: 125, output: 120)
- Token Efficiency Ratio: 3.76
- Cost per Response: $0.008
- Avg Latency: 2.3 seconds

### Reliability Metrics
- Format Compliance: 99%
- Inter-run Agreement: 0.89 (cosine similarity)
- Failure Rate: 0.5%
- Quality Variance: œÉ = 0.12

### Business Impact
- Customer Satisfaction: 4.6/5.0
- Response Time Reduction: 85%
- Escalation Rate: 8% (down from 35%)
- Monthly Cost Savings: $45,000

### Overall Score: 88/100 ‚úì
Recommendation: Deploy to production with monitoring`,
          description: 'A comprehensive metrics dashboard provides actionable insights for prompt optimization.'
        }
      ]
    },
    {
      id: 'evaluation-methodologies',
      title: 'Evaluation Methodologies',
      content: `## Systematic Prompt Evaluation

### 1. Automated Evaluation

**Rule-Based Checking:**
- Format verification using regex patterns
- Keyword presence detection
- Length constraint validation
- Structure compliance testing

**Model-Based Evaluation:**
- Using GPT-4 as an evaluator
- Embedding similarity for semantic matching
- Classification models for quality assessment
- Sentiment analysis for tone verification

**Programmatic Testing:**
\`\`\`python
def evaluate_prompt_output(output, requirements):
    scores = {}
    scores['format'] = check_format_compliance(output)
    scores['keywords'] = verify_keywords(output, requirements)
    scores['length'] = validate_length(output, requirements)
    scores['quality'] = assess_quality_llm(output)
    return aggregate_scores(scores)
\`\`\`

### 2. Human Evaluation

**Expert Assessment:**
- Domain experts rate accuracy
- Writing professionals assess quality
- Target users evaluate usefulness
- Blind comparison studies

**Evaluation Criteria:**
| Criterion | Weight | Scale |
|-----------|--------|--------|
| Accuracy | 30% | 1-5 |
| Usefulness | 25% | 1-5 |
| Clarity | 20% | 1-5 |
| Completeness | 15% | 1-5 |
| Style | 10% | 1-5 |

**Inter-Rater Reliability:**
- Use multiple evaluators
- Calculate Cohen's Kappa for agreement
- Resolve discrepancies through discussion
- Document evaluation guidelines

### 3. A/B Testing Framework

**Experimental Design:**
1. **Hypothesis**: Define expected improvement
2. **Variables**: Identify what to change
3. **Control**: Baseline prompt performance
4. **Treatment**: Modified prompt version
5. **Sample Size**: Statistical power calculation
6. **Duration**: Sufficient data collection
7. **Analysis**: Statistical significance testing

**Sample Size Calculation:**
\`n = 2 √ó (ZŒ± + ZŒ≤)¬≤ √ó œÉ¬≤ / Œ¥¬≤\`

Where:
- n = sample size per group
- ZŒ± = critical value for significance level
- ZŒ≤ = critical value for power
- œÉ = standard deviation
- Œ¥ = minimum detectable difference

### 4. Regression Testing

**Prompt Regression Suite:**
- Canonical test cases
- Edge case collection
- Performance benchmarks
- Quality thresholds

**Continuous Evaluation:**
\`\`\`yaml
test_suite:
  basic_tasks:
    - summarization:
        min_accuracy: 0.90
    - classification:
        min_f1: 0.85
  edge_cases:
    - empty_input:
        should: "handle gracefully"
    - maximum_length:
        should: "maintain quality"
\`\`\`

### 5. Production Monitoring

**Real-time Metrics:**
- Live quality scoring
- Anomaly detection
- Drift monitoring
- User feedback integration

**Alert Thresholds:**
- Quality drop > 10%
- Error rate > 1%
- Latency > 5 seconds
- Cost spike > 20%`,
      examples: [
        {
          title: 'A/B Testing Example',
          code: `# A/B Test: Prompt Optimization for Code Generation

## Hypothesis
Adding explicit constraints will improve code quality by 25%

## Setup
- Control (A): Basic code generation prompt
- Treatment (B): Prompt with constraints and structure
- Sample Size: 500 tasks per variant
- Duration: 5 days
- Success Metric: Code passes automated tests

## Variant A (Control)
"Write a Python function to [TASK]"

## Variant B (Treatment)
"Write a Python function to [TASK].
Requirements:
- Include type hints
- Add comprehensive docstring
- Handle edge cases
- Include error handling
- Follow PEP 8 style"

## Results
| Metric | Variant A | Variant B | Improvement |
|--------|-----------|-----------|-------------|
| Test Pass Rate | 68% | 89% | +30.9% ‚úì |
| Type Hints | 45% | 98% | +117.8% |
| Error Handling | 34% | 91% | +167.6% |
| Avg Tokens | 180 | 210 | +16.7% |
| User Rating | 3.2/5 | 4.5/5 | +40.6% |

## Statistical Analysis
- Chi-square test: p < 0.001 (highly significant)
- Effect size (Cohen's d): 0.84 (large)
- Confidence interval: [25.3%, 36.5%]

## Decision: Deploy Variant B ‚úì`,
          description: 'Systematic A/B testing provides statistical evidence for prompt improvements.'
        }
      ]
    },
    {
      id: 'benchmark-design',
      title: 'Designing Prompt Benchmarks',
      content: `## Creating Robust Evaluation Benchmarks

### Benchmark Components

**1. Task Taxonomy:**
- **Information Extraction**: Pull specific data from text
- **Generation**: Create new content
- **Analysis**: Evaluate and interpret
- **Transformation**: Convert between formats
- **Reasoning**: Solve problems requiring logic
- **Classification**: Categorize inputs

**2. Difficulty Levels:**
| Level | Characteristics | Example Tasks |
|-------|----------------|---------------|
| Basic | Single-step, clear requirements | Summarize in 3 sentences |
| Intermediate | Multi-step, some ambiguity | Analyze and compare two approaches |
| Advanced | Complex reasoning, nuanced output | Design system architecture with tradeoffs |
| Expert | Domain expertise, creative solutions | Novel algorithm development |

**3. Evaluation Datasets:**

**Curated Test Sets:**
- Representative samples from production
- Edge cases and corner scenarios
- Adversarial examples
- Golden standard outputs

**Synthetic Datasets:**
- Programmatically generated tests
- Controlled difficulty progression
- Systematic coverage of variations
- Scalable to thousands of examples

### Benchmark Metrics

**Holistic Scoring Framework:**

\`\`\`python
class PromptBenchmark:
    def __init__(self):
        self.metrics = {
            'accuracy': AccuracyMetric(),
            'efficiency': EfficiencyMetric(),
            'robustness': RobustnessMetric(),
            'generalization': GeneralizationMetric()
        }

    def evaluate(self, prompt, test_suite):
        results = {}
        for test in test_suite:
            output = run_prompt(prompt, test.input)
            for metric_name, metric in self.metrics.items():
                score = metric.calculate(output, test.expected)
                results[f"{test.id}_{metric_name}"] = score
        return aggregate_results(results)
\`\`\`

### Industry Standard Benchmarks

**Existing Benchmarks to Consider:**

| Benchmark | Focus Area | Metrics |
|-----------|------------|---------|
| GLUE | Language Understanding | Accuracy, F1 |
| SuperGLUE | Advanced Language Tasks | Various task-specific |
| MMLU | Multitask Knowledge | Accuracy across domains |
| HumanEval | Code Generation | Pass@k rate |
| TruthfulQA | Factual Accuracy | Truth score |
| HellaSwag | Commonsense Reasoning | Accuracy |

### Custom Benchmark Creation

**Step-by-Step Process:**

1. **Define Objectives**
   - What capabilities to measure?
   - What constitutes success?
   - How to ensure fairness?

2. **Collect Data**
   - Real production examples
   - Expert-annotated samples
   - Synthetic test cases
   - Adversarial inputs

3. **Establish Baselines**
   - Human performance
   - Previous prompt versions
   - Competitor benchmarks
   - Theoretical limits

4. **Create Evaluation Pipeline**
   - Automated testing infrastructure
   - Scoring algorithms
   - Statistical analysis
   - Visualization dashboard

5. **Validate and Iterate**
   - Cross-validation
   - Expert review
   - Correlation with business metrics
   - Regular updates`,
      examples: [
        {
          title: 'Custom Benchmark Implementation',
          code: `# Customer Support Prompt Benchmark v1.0

## Test Categories

### 1. Simple Queries (30% weight)
test_simple = [
    {
        "id": "simple_001",
        "input": "How do I reset my password?",
        "expected_elements": ["step-by-step", "security", "link"],
        "max_tokens": 150,
        "tone": "helpful"
    },
    # ... 50 more simple tests
]

### 2. Complex Issues (40% weight)
test_complex = [
    {
        "id": "complex_001",
        "input": "I was charged twice, the product arrived damaged,
                  and customer service hung up on me.",
        "expected_elements": ["apology", "refund", "replacement", "escalation"],
        "priority_order": ["refund", "replacement", "compensation"],
        "max_tokens": 300,
        "tone": "empathetic"
    },
    # ... 50 more complex tests
]

### 3. Edge Cases (30% weight)
test_edge = [
    {
        "id": "edge_001",
        "input": "HELP!!!!! URGENT!!! üò°üò°üò°",
        "expected_behavior": "remain calm, professional response",
        "should_not": ["match emotion", "use caps"],
        "max_tokens": 200
    },
    # ... 30 edge case tests
]

## Scoring Algorithm

def benchmark_score(prompt_template):
    scores = {
        'simple': run_tests(prompt_template, test_simple),
        'complex': run_tests(prompt_template, test_complex),
        'edge': run_tests(prompt_template, test_edge)
    }

    weighted_score = (
        scores['simple'] * 0.3 +
        scores['complex'] * 0.4 +
        scores['edge'] * 0.3
    )

    return {
        'overall': weighted_score,
        'breakdown': scores,
        'pass': weighted_score > 0.85
    }

## Baseline Performance
- Human Agent: 92% average score
- Current Prompt v1.8: 81% average score
- Target for v2.0: 88% average score

## Results Visualization
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Benchmark Results               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Simple:   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 89%    ‚îÇ
‚îÇ Complex:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 76%    ‚îÇ
‚îÇ Edge:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 95%    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Overall:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 85% ‚úì  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
          description: 'A comprehensive benchmark provides objective measurement of prompt improvements.'
        }
      ]
    },
    {
      id: 'statistical-analysis',
      title: 'Statistical Analysis for Prompt Engineering',
      content: `## Statistical Rigor in Prompt Evaluation

### Hypothesis Testing

**Comparing Prompt Versions:**

**Null Hypothesis (H‚ÇÄ):** No difference between prompts
**Alternative Hypothesis (H‚ÇÅ):** New prompt performs better

**Appropriate Statistical Tests:**

| Data Type | Test | When to Use |
|-----------|------|-------------|
| Binary outcomes | Chi-square | Success/failure tasks |
| Continuous scores | t-test | Quality ratings |
| Multiple groups | ANOVA | >2 prompt variants |
| Paired observations | Paired t-test | Same input, different prompts |
| Non-parametric | Mann-Whitney U | Non-normal distributions |

### Effect Size Calculation

**Cohen's d for Continuous Metrics:**
\`d = (Œº‚ÇÅ - Œº‚ÇÇ) / œÉ_pooled\`

**Interpretation:**
- Small: d = 0.2
- Medium: d = 0.5
- Large: d = 0.8

**Practical Significance:**
Beyond statistical significance, consider:
- Business impact threshold
- Cost-benefit ratio
- Implementation complexity

### Confidence Intervals

**95% Confidence Interval:**
\`CI = Œº ¬± 1.96 √ó (œÉ/‚àön)\`

**Interpretation:**
"We are 95% confident the true performance lies within this range"

### Power Analysis

**Determining Required Sample Size:**

For 80% power at Œ± = 0.05:
\`n = 16œÉ¬≤/Œ¥¬≤\` (per group)

**Factors Affecting Power:**
- Effect size (larger = more power)
- Sample size (more = more power)
- Significance level (higher Œ± = more power)
- Variance (lower = more power)

### Multiple Comparison Correction

When testing multiple prompts simultaneously:

**Bonferroni Correction:**
\`Œ±_adjusted = Œ± / m\`

Where m = number of comparisons

**False Discovery Rate (FDR):**
Benjamini-Hochberg procedure for controlling FDR

### Variance Analysis

**Understanding Output Variability:**

**Within-Prompt Variance:**
- Temperature settings impact
- Random seed effects
- Input sensitivity

**Between-Prompt Variance:**
- Structural differences
- Instruction clarity
- Constraint specification

**Variance Decomposition:**
\`œÉ¬≤_total = œÉ¬≤_prompt + œÉ¬≤_input + œÉ¬≤_random\`

### Time Series Analysis

**Monitoring Prompt Performance Over Time:**

**Trend Detection:**
- Moving averages
- Linear regression
- Change point detection

**Seasonality:**
- Daily patterns
- Weekly cycles
- Event-driven changes

**Anomaly Detection:**
- Z-score method
- Isolation forests
- LSTM-based detection`,
      examples: [
        {
          title: 'Statistical Analysis in Practice',
          code: `import numpy as np
from scipy import stats
import pandas as pd

# Comparing Two Prompt Versions

# Data Collection
prompt_a_scores = [0.82, 0.79, 0.85, 0.81, 0.78, 0.83, 0.80, 0.84]
prompt_b_scores = [0.89, 0.91, 0.88, 0.92, 0.90, 0.87, 0.93, 0.89]

# Descriptive Statistics
stats_a = {
    'mean': np.mean(prompt_a_scores),
    'std': np.std(prompt_a_scores),
    'n': len(prompt_a_scores)
}

stats_b = {
    'mean': np.mean(prompt_b_scores),
    'std': np.std(prompt_b_scores),
    'n': len(prompt_b_scores)
}

# Hypothesis Testing
t_stat, p_value = stats.ttest_ind(prompt_a_scores, prompt_b_scores)

# Effect Size (Cohen's d)
pooled_std = np.sqrt((stats_a['std']**2 + stats_b['std']**2) / 2)
cohens_d = (stats_b['mean'] - stats_a['mean']) / pooled_std

# Confidence Intervals
ci_a = stats.t.interval(0.95, stats_a['n']-1,
                        stats_a['mean'],
                        stats_a['std']/np.sqrt(stats_a['n']))
ci_b = stats.t.interval(0.95, stats_b['n']-1,
                        stats_b['mean'],
                        stats_b['std']/np.sqrt(stats_b['n']))

# Results Report
print(f"""
Statistical Analysis Report
===========================
Prompt A Performance:
  Mean: {stats_a['mean']:.3f}
  95% CI: [{ci_a[0]:.3f}, {ci_a[1]:.3f}]

Prompt B Performance:
  Mean: {stats_b['mean']:.3f}
  95% CI: [{ci_b[0]:.3f}, {ci_b[1]:.3f}]

Hypothesis Test:
  t-statistic: {t_stat:.3f}
  p-value: {p_value:.4f}
  Significant: {'Yes ‚úì' if p_value < 0.05 else 'No ‚úó'}

Effect Size:
  Cohen's d: {cohens_d:.3f}
  Interpretation: {'Large' if cohens_d > 0.8 else 'Medium' if cohens_d > 0.5 else 'Small'}

Recommendation:
  {'Deploy Prompt B' if p_value < 0.05 and cohens_d > 0.5 else 'Continue testing'}
""")

# Power Analysis for Future Tests
from statsmodels.stats.power import ttest_power

required_n = ttest_power(cohens_d, power=0.8, alpha=0.05, alternative='two-sided')
print(f"\\nSample size needed for future tests: {int(np.ceil(required_n))} per group")`,
          description: 'Statistical analysis provides rigorous evidence for prompt selection decisions.'
        }
      ]
    },
    {
      id: 'optimization-pipeline',
      title: 'Building Prompt Optimization Pipelines',
      content: `## Automated Prompt Optimization

### Continuous Improvement Pipeline

**Architecture Overview:**

\`\`\`
Data Collection ‚Üí Evaluation ‚Üí Analysis ‚Üí Optimization ‚Üí Deployment
      ‚Üë                                                        ‚Üì
      ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Feedback Loop ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
\`\`\`

### Pipeline Components

**1. Data Collection Layer:**
- Production prompt logs
- User feedback signals
- Error reports
- Performance metrics
- A/B test results

**2. Evaluation Engine:**
\`\`\`python
class EvaluationEngine:
    def __init__(self):
        self.metrics = MetricsCalculator()
        self.validators = ValidationSuite()
        self.benchmarks = BenchmarkRunner()

    async def evaluate_prompt(self, prompt, test_data):
        results = await asyncio.gather(
            self.metrics.calculate_all(prompt, test_data),
            self.validators.validate_all(prompt, test_data),
            self.benchmarks.run_all(prompt)
        )
        return self.aggregate_results(results)
\`\`\`

**3. Optimization Strategies:**

**Grid Search:**
- Systematic parameter exploration
- Exhaustive but expensive
- Good for small parameter spaces

**Bayesian Optimization:**
- Intelligent search using priors
- Efficient for expensive evaluations
- Handles continuous parameters well

**Genetic Algorithms:**
- Evolution-inspired optimization
- Good for complex prompt structures
- Handles discrete and continuous variables

**Reinforcement Learning:**
- Learn from interaction feedback
- Adapts to changing requirements
- Handles sequential decisions

**4. Deployment Automation:**

\`\`\`yaml
deployment_pipeline:
  stages:
    - validation:
        tests: ["unit", "integration", "regression"]
        threshold: 0.95
    - canary:
        traffic_percentage: 5
        duration: "1h"
        metrics: ["error_rate", "latency", "quality"]
    - gradual_rollout:
        increments: [10, 25, 50, 100]
        interval: "30m"
    - monitoring:
        alerts: ["quality_drop", "error_spike", "cost_increase"]
\`\`\`

### Optimization Algorithms

**1. Prompt Evolution Algorithm:**

\`\`\`python
def evolve_prompt(base_prompt, iterations=50):
    population = generate_variations(base_prompt, n=20)

    for generation in range(iterations):
        # Evaluate fitness
        fitness_scores = evaluate_population(population)

        # Selection
        parents = select_top_performers(population, fitness_scores, k=10)

        # Crossover
        offspring = crossover(parents)

        # Mutation
        mutated = mutate(offspring, rate=0.1)

        # New generation
        population = parents + mutated

        # Track best
        best_prompt = population[np.argmax(fitness_scores)]

    return best_prompt
\`\`\`

**2. Automated Prompt Refinement:**

\`\`\`python
class PromptRefiner:
    def refine(self, prompt, feedback):
        issues = self.identify_issues(feedback)

        refinements = {
            'low_accuracy': self.add_constraints,
            'poor_format': self.clarify_structure,
            'inconsistent': self.increase_specificity,
            'verbose': self.add_length_limits,
            'off_topic': self.improve_focus
        }

        for issue in issues:
            if issue in refinements:
                prompt = refinements[issue](prompt)

        return prompt
\`\`\`

### Performance Tracking Dashboard

**Key Metrics to Track:**

| Metric | Frequency | Alert Threshold |
|--------|-----------|-----------------|
| Success Rate | Real-time | < 90% |
| Avg Quality Score | Hourly | < 4.0/5 |
| Token Efficiency | Daily | > 500 avg |
| Cost per Query | Daily | > $0.02 |
| User Satisfaction | Weekly | < 4.2/5 |
| Error Rate | Real-time | > 1% |

**Visualization Components:**
- Time series plots for trends
- Heatmaps for prompt variant performance
- Funnel analysis for multi-step prompts
- Distribution plots for response quality
- Correlation matrices for metric relationships`,
      examples: [
        {
          title: 'Complete Optimization Pipeline Example',
          code: `# Automated Prompt Optimization Pipeline
# Email Response Generator Optimization

## 1. Initial Prompt
base_prompt = """
Respond to customer email professionally.
Address their concerns and provide solution.
"""

## 2. Generate Variations
variations = [
    # Add role
    "As a customer service expert, respond to customer email...",

    # Add structure
    "Respond to customer email:\n1. Acknowledge concern\n2. Provide solution\n3. Next steps",

    # Add constraints
    "Respond to customer email (max 150 words, professional tone, include apology if needed)",

    # Combine improvements
    "As a senior customer service representative:\n1. Acknowledge the issue\n2. Apologize sincerely\n3. Provide clear solution\n4. Offer follow-up\nTone: Professional, empathetic\nLength: 100-150 words"
]

## 3. A/B Testing Results
results = {
    "base": {"quality": 3.2, "satisfaction": 3.5, "tokens": 180},
    "role": {"quality": 3.8, "satisfaction": 3.9, "tokens": 185},
    "structure": {"quality": 4.1, "satisfaction": 4.2, "tokens": 160},
    "constraints": {"quality": 4.0, "satisfaction": 4.0, "tokens": 145},
    "combined": {"quality": 4.6, "satisfaction": 4.7, "tokens": 155}
}

## 4. Statistical Analysis
# Winner: Combined approach
# p-value: 0.001 (highly significant)
# Effect size: 1.2 (very large)
# ROI: 35% reduction in escalations

## 5. Deployment Configuration
deployment = {
    "prompt_version": "v2.0-combined",
    "rollout_strategy": "canary",
    "initial_traffic": "5%",
    "success_criteria": {
        "min_quality": 4.5,
        "max_tokens": 160,
        "error_rate": "<0.01"
    },
    "monitoring": {
        "dashboards": ["quality", "efficiency", "errors"],
        "alerts": ["quality < 4.3", "tokens > 170", "errors > 0.02"],
        "review_frequency": "daily"
    }
}

## 6. Continuous Optimization
optimization_schedule = {
    "daily": ["collect_feedback", "calculate_metrics"],
    "weekly": ["run_ab_tests", "analyze_results"],
    "monthly": ["major_revision", "benchmark_comparison"],
    "quarterly": ["strategy_review", "architecture_update"]
}

## 7. Results After 30 Days
final_metrics = {
    "quality_improvement": "+43%",
    "token_reduction": "-14%",
    "cost_savings": "$12,000/month",
    "customer_satisfaction": "4.7/5.0 (up from 3.5)",
    "escalation_rate": "5% (down from 15%)"
}`,
          description: 'A complete pipeline automates the entire prompt optimization lifecycle.'
        }
      ]
    }
  ],

  summary: [
    'Effective prompt evaluation requires comprehensive metrics across performance, efficiency, reliability, and business impact',
    'Systematic evaluation methodologies include automated testing, human evaluation, A/B testing, and production monitoring',
    'Statistical rigor through hypothesis testing, effect sizes, and confidence intervals ensures reliable decisions',
    'Custom benchmarks provide objective measurement tailored to specific use cases',
    'Automated optimization pipelines enable continuous improvement through data-driven refinement',
    'Successful prompt engineering combines quantitative metrics with qualitative insights',
    'ROI measurement connects prompt improvements to business outcomes'
  ],

  checkYourUnderstanding: [
    {
      question: 'What are the four main categories of prompt evaluation metrics?',
      answer: 'The four categories are: 1) Task Performance (accuracy, quality), 2) Efficiency (tokens, latency, cost), 3) Reliability (consistency, format stability), and 4) Business Impact (user satisfaction, time saved, error reduction).'
    },
    {
      question: 'When should you use A/B testing versus regression testing for prompts?',
      answer: 'Use A/B testing when comparing prompt variations to determine which performs better on specific metrics. Use regression testing to ensure prompt changes don\'t break existing functionality or degrade performance on established benchmarks.'
    },
    {
      question: 'What is Cohen\'s d and how do you interpret it?',
      answer: 'Cohen\'s d measures effect size - the standardized difference between two means. Values: 0.2 = small effect, 0.5 = medium effect, 0.8 = large effect. It helps determine if statistically significant results are practically meaningful.'
    },
    {
      question: 'What components should an automated prompt optimization pipeline include?',
      answer: 'Essential components: 1) Data collection layer for logs and metrics, 2) Evaluation engine for testing and benchmarking, 3) Optimization algorithms (grid search, Bayesian optimization, etc.), 4) Deployment automation with canary releases and monitoring, 5) Feedback loops for continuous improvement.'
    }
  ],

  nextSteps: [
    'Design custom metrics for your specific use case',
    'Build an A/B testing framework for prompt variations',
    'Create automated evaluation pipelines',
    'Implement statistical analysis for prompt comparisons',
    'Develop comprehensive benchmarks for your domain',
    'Set up production monitoring and alerting systems'
  ]
};